\documentclass[11pt,a4paper]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{cleveref}

% Page geometry
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
    headheight=24pt
}

% Colors
\definecolor{leshouches}{RGB}{25,55,95}
\definecolor{lightgray}{RGB}{240,240,240}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{leshouches}{\small\textsc{Les Houches School of Physics}}}
\fancyhead[R]{\textcolor{leshouches}{\small Lecture \thelecturenumber}}
\fancyfoot[C]{\textcolor{leshouches}{\thepage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{leshouches}\leaders\hrule height \headrulewidth\hfill}}

% Section formatting
\titleformat{\section}
{\Large\bfseries\color{leshouches}}
{\thesection}{1em}{}

\titleformat{\subsection}
{\large\bfseries\color{leshouches}}
{\thesubsection}{1em}{}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{remark}{Remark}

% Custom commands
\makeatletter
\newcommand{\lecturetitle}[1]{\gdef\@lecturetitle{#1}}
\newcommand{\lecturer}[1]{\gdef\@lecturer{#1}}
\newcommand{\lecturedate}[1]{\gdef\@lecturedate{#1}}
\newcommand{\lecturenumber}[1]{\gdef\@lecturenumber{#1}\gdef\thelecturenumber{#1}}

% Default values
\gdef\@lecturetitle{Lecture Title}
\gdef\@lecturer{Lecturer Name}
\gdef\@lecturedate{\today}
\gdef\@lecturenumber{1}

% Title formatting
\renewcommand{\maketitle}{
    \begin{center}
        {\Large\textcolor{leshouches}{\textbf{Les Houches School of Physics}}}\\[0.5em]
        {\large Summer School on Dark Universe, 2025}\\[1.5em]
        {\huge\textbf{\@lecturetitle}}\\[0.5em]
        {\large Lecture \@lecturenumber}\\[1em]
        {\large\textit{\@lecturer}}\\[0.5em]
        {\@lecturedate}
    \end{center}
    \vspace{1em}
    \hrule
    \vspace{1.5em}
}
\makeatother

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=leshouches,
    citecolor=leshouches,
    urlcolor=leshouches,
    pdftitle={Simulation-Based Inference for Cosmology - Les Houches 2025},
    pdfauthor={François Lanusse, CNRS/CEA Paris-Saclay}
}

% Document begins here
\begin{document}

% Set lecture information
\lecturetitle{Simulation-Based Inference for Cosmology}
\lecturer{François Lanusse, CNRS/CEA Paris-Saclay}
\lecturedate{Summer 2025}
\lecturenumber{1}

\maketitle

\section*{Introduction}

Simulation-Based Inference (SBI) addresses a fundamental challenge in modern cosmological data analysis: performing Bayesian inference when the likelihood function $p(d|\theta)$ is intractable or computationally prohibitive to evaluate. Traditional approaches rely on analytical likelihood expressions or approximations that become inadequate for complex, high-dimensional data such as cosmic web fields, galaxy clustering, or gravitational lensing maps.

The key insight of SBI is to \textbf{replace explicit likelihood} evaluations required by traditional MCMC, \textbf{with an implicit likelihood learned from simulations}. In other words, rather than deriving the likelihood of the data $p(d|\theta)$ analytically, simulations can be used to generate synthetic training data that captures the relationship between parameters $\theta$ and observations $d$. Neural networks can then be used to approximate the relevant distributions (whether the posterior $p(\theta|d)$, likelihood $p(d|\theta)$, or likelihood ratios) directly from this simulation data.

This approach has proven particularly powerful in cosmology, where sophisticated N-body simulations and hydrodynamical codes can generate realistic synthetic observations, but the underlying physics is too complex for tractable likelihood expressions. SBI enables inference over high-dimensional parameter spaces using the full information content of complex datasets, moving beyond traditional summary statistics.

In these lecture notes, we will cover the connection between SBI and other forms of Bayesian inference, the theoretical foundations of neural density estimation powering SBI, and its practical applications in cosmological inference problems. We will also discuss the challenges and best practices for implementing SBI in real-world scenarios.


\section{MCMC, SBI, Full Field Inference... What is the difference?}

\subsection{Fundamental notations and definitions for Bayesian Inference}

Let us start by establishing the standard framework for Bayesian inference. Let $\theta \in \Theta \subseteq \mathbb{R}^p$ denote a vector of parameters of interest, and $d \in \mathcal{D}$ represent observed data. The fundamental quantities in Bayesian inference are:

\begin{itemize}
    \item \textbf{Prior distribution}: $p(\theta)$ encodes our beliefs about parameters before observing data
    \item \textbf{Likelihood function}: $p(d|\theta)$ specifies the probability of observing data $d$ given parameters $\theta$
    \item \textbf{Posterior distribution}: $p(\theta|d)$ represents our updated beliefs about parameters after observing data
    \item \textbf{Evidence (marginal likelihood)}: $p(d) = \int p(d|\theta)p(\theta) \, d\theta$ normalizes the posterior
\end{itemize}

Given these definitions, the famous Bayes' theorem connects these quantities according to:
\begin{equation}
    \boxed{p(\theta|d) = \frac{p(d|\theta)p(\theta)}{p(d)}}
\end{equation}

No matter the inference strategy, these definitions remain the same. SBI will differ from other Bayesian inference techniques only in the strategy used to estimate these distributions. 

\subsection{Explicit Inference}

\textbf{Explicit inference} forms the foundation of classical Bayesian computation and encompasses all approaches where the \textbf{likelihood function $p(d|\theta)$ can be evaluated point-wise} for any given combination of data $d$ and parameters $\theta$. This seemingly simple requirement enables the entire machinery of Markov Chain Monte Carlo (MCMC) algorithms that have dominated Bayesian inference for decades.

To understand why likelihood evaluation is crucial, consider how MCMC algorithms explore the posterior distribution. Whether using the Metropolis-Hastings algorithm, Gibbs sampling, or more sophisticated variants, these methods fundamentally rely on computing \textbf{likelihood ratios} to make accept-reject decisions. At each step, a proposal $\theta'$ is compared to the current state $\theta$ through the ratio:
\begin{equation}
    r = \frac{p(\theta'|d)}{p(\theta|d)} = \frac{p(d|\theta')p(\theta')}{p(d|\theta)p(\theta)}
\end{equation}
Without the ability to evaluate the likelihoods $p(d|\theta)$ and $p(d|\theta')$ at two different points in parameter space, this ratio cannot be computed, and the MCMC chain cannot advance. This fundamental dependency explains why explicit inference has been the dominant paradigm in computational statistics.

In practice, evaluating likelihoods for raw cosmological data presents a fundamental challenge. Consider a weak lensing survey observing shear measurements $\gamma_{i}$ for millions of galaxies across the sky. The likelihood $p(\{\gamma_i\}|\theta)$ for these raw measurements involves complex correlations due to cosmic structure, survey geometry, and measurement uncertainties—making analytical evaluation intractable. The same problem arises for galaxy positions, CMB temperature maps, or any high-dimensional cosmological dataset.

This tractability issue drove cosmologists to adopt \textbf{summary statistics} as an ingenious workaround. Instead of working with millions of individual measurements, we compress the data into a manageable set of statistics for which analytical likelihoods can be derived. The \textbf{two-point correlation function} became the workhorse of cosmological analysis precisely because its likelihood could be written down. For weak lensing, shear measurements from millions of galaxies are compressed into correlation functions $\xi_+(\theta)$ and $\xi_-(\theta)$, whose likelihood can be modeled as multivariate Gaussian:
\begin{equation}
    p(\xi_{\text{obs}}|\theta) = \mathcal{N}(\xi_{\text{obs}}; \xi_{\text{theory}}(\theta), C)
\end{equation}
where $\xi_{\text{theory}}(\theta)$ comes from analytical models (linear perturbation theory, halo models) and $C$ represents the covariance matrix. This Gaussian assumption is justified by the Central Limit Theorem when averaging over many independent modes, making the likelihood both tractable and well-understood.

This framework has enabled precise cosmological parameter constraints from Large Scale Structure surveys like DES, KiDS, and HSC. However, it comes at the cost a fundamental limitation: potential \textbf{information loss}. By compressing millions of measurements into a handful of correlation function bins, we may discard a significant amount of information contained in higher-order statistics, non-Gaussian features, and spatial correlations that cannot be captured by two-point functions, which are only theoretically optimal for the analysis of Gaussian fields.

To access the \textit{full information content} of a given survey while maintaining explicit inference, we must abandon summary statistics and instead model the measured fields in high dimension and in all their complexity, which is usualy referred to as \textbf{full-field inference}. This will require constructing complex models that represent the \textbf{complete data generation process} behind the observations, in other words, a forward simulation of the measured high-dimensional data.

Performing explicit inference over such models is possible but requires the introduction of a set of \textbf{latent variables} $z$ representing unobserved quantities like the true underlying cosmic fields. \textbf{The generative model becomes hierarchical} and can be expressed as:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    z|\theta &\sim p(z|\theta) \quad \text{(latent cosmic field)} \\
    d|z &\sim p(d|z) \quad \text{(observation process)}
\end{align}

The joint posterior over both parameters and latent variables is:
\begin{equation}
    p(\theta, z|d) \propto p(d|z)p(z|\theta)p(\theta)
\end{equation}

This framework remains \textbf{explicitly tractable} as long as each component can be evaluated: $p(d|z)$ models the observation process (e.g., Poisson noise, instrumental systematics), $p(z|\theta)$ represents the physical field generation process, and $p(\theta)$ encodes prior knowledge. Compared to summary-statistics-based explicit inference, we now need to infer the set of latent variable $z$ in addition to the comological parameters $\theta$ which can become very challening whe $z$ is high-dimensional. 

A paradigmatic example is the \textbf{BORG algorithm} (Bayesian Origin Reconstruction from Galaxies, see Florent's lecture in this school). BORG performs inference on the initial density field of the universe given observed galaxy positions. The hierarchical model structure is:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    \delta_0 &\sim \mathcal{N}(0, P(\theta)) \quad \text{(initial density field)} \\
    \delta(z) &= \mathcal{F}[\delta_0, \theta] \quad \text{(gravitational evolution)} \\
    n_g &= \text{Poisson}(\rho_g[\delta(z)]) \quad \text{(galaxy counts)}
\end{align}

Here, $\mathcal{F}$ represents forward modeling through Lagrangian Perturbation Theory (LPT), and $\rho_g$ is the biased galaxy density field. Crucially, even though $\mathcal{F}$ involves complex physics, the likelihood remains explicit because: (1) LPT provides an analytical (though approximate) mapping from initial to final conditions, (2) all intermediate variables ($\delta_0$, $\delta(z)$) are tracked explicitly, and (3) the observation model $p(n_g|\delta(z))$ is analytically tractable. BORG can simultaneously reconstruct the cosmic web and constrain cosmological parameters while utilizing the full spatial information in galaxy surveys.

From an information-theoretic perspective, explicit hierarchical inference represents the \textbf{theoretically optimal approach} to parameter estimation. By modeling the complete generative process without information-losing compression, these methods can achieve the theoretical minimum possible uncertainties (the Cramér-Rao bound). 

However, this theoretical optimality comes with important practical challenges. 

The first is \textbf{computational scaling}: the dimensionality of latent variable spaces grows rapidly with field resolution. A modest $128^3$ cosmic web simulation involves $\sim 2 \times 10^6$ latent variables, making standard MCMC inefficient. High-dimensional sampling requires sophisticated algorithms like Hamiltonian Monte Carlo (HMC) that can explore the posterior efficiently.

The second challenge involves \textbf{differentiability requirements}: algorithms like HMC require computing gradients of the log-posterior with respect to all variables, including the high-dimensional latent field. This necessitates that the forward model $p(z|\theta)$ be differentiable, a strong constraint that excludes many realistic simulation codes.

The third challenge is \textbf{model fidelity}: explicit models must make approximations to remain tractable. BORG's use of LPT, while differentiable, becomes inaccurate in highly nonlinear regimes. The fundamental tension between model complexity and computational tractability limits the realism achievable in explicit frameworks.

\bigskip

Wouldn't it be nice if we could perform Full-Field inference without running into these difficulties intrinsic to high-dimensional inference?

\subsection{Implicit Inference}

Having established that explicit inference reaches its limits when forward models become too complex to decompose into tractable probabilistic components, we now turn to scenarios where the likelihood $p(d|\theta)$ becomes fundamentally unevaluable. This transition from explicit to implicit inference represents a critical juncture in modern cosmological analysis.

We define \textbf{implicit inference} as any Bayesian inference approach where the likelihood function $p(d|\theta)$ cannot be evaluated directly, but where we can generate samples from the joint distribution $p(\theta, d)$ through forward simulations. In contrast to explicit methods that require point-wise likelihood evaluation, implicit methods work with \textbf{implicit distributions}: probability distributions that are defined through a generative process (such as running a simulation) rather than through an analytical expression that can be evaluated at arbitrary points.

This paradigm becomes necessary when the complexity of our models and simulations precludes explicit inference. This occurs in two main scenarios where the computational or mathematical requirements exceed what explicit methods can handle.

The first challenge arises from \textbf{high-dimensional inference problems}. When dealing with full-field inference in very high dimensions, even explicit hierarchical models like BORG face computational challenges. Sampling efficiently in spaces with millions of latent variables remains difficult, and the required computational resources can become prohibitive.

The second limitation involves \textbf{non-differentiable or intractable forward models}. Many realistic cosmological simulations cannot be easily decomposed into the tractable probabilistic components required for explicit inference. For instance, differentiable models of full hydrodynamical simulations with realistic baryonic feedback do not yet exist, making it impossible to apply gradient-based sampling methods or even evaluate the likelihood function analytically.

\subsubsection{The Fundamental Insight of Simulation-Based Inference}

Faced with these challenges, SBI recognizes a crucial asymmetry: \textbf{while we cannot evaluate} $p(d|\theta)$ \textbf{directly, we can generate samples from it effortlessly}. This observation leads to the central insight that transforms an intractable inference problem into a tractable machine learning problem.

The key recognition is that for any parameter configuration $\theta$, we can generate synthetic observations through the simple procedure:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(sample from prior)} \\
    d &= \mathcal{S}(\theta, \epsilon) \quad \text{(run forward simulation)}
\end{align}

This generates pairs $\{(\theta_i, d_i)\}_{i=1}^N$ that are exact samples from the joint distribution $p(\theta, d)$, despite our inability to evaluate the likelihood analytically. The forward simulation $\mathcal{S}$ can be arbitrarily complex—involving hydrodynamical physics, stochastic star formation, Monte Carlo radiative transfer, etc. as long as it can be executed.

Having access to samples from $p(\theta, d)$, all that remains it the ability to estimate conditional density functions like $p(d|\theta)$ or $p(\theta|d)$ given $\{(\theta_i, d_i)\}_{i=1}^N$.

\subsubsection{Neural Networks as Distribution Approximators}

This approach transforms Bayesian inference from an analytical derivation problem into a \textbf{supervised learning problem}. Neural networks serve as flexible function approximators that can learn complex, high-dimensional probability distributions from training data. The universal approximation theorem guarantees that sufficiently wide neural networks can approximate any continuous function to arbitrary precision, making them ideal candidates for this task.

The training process follows standard machine learning principles: we maximize the likelihood of observed simulation pairs under the neural network's parametric distribution. For example, to learn the posterior $p(\theta|d)$, we train a neural network $f_\phi(d)$ that outputs the parameters of a probability distribution over $\theta$, optimizing:
\begin{equation}
    \phi^* = \arg\max_\phi \sum_{i=1}^N \log p_\phi(\theta_i | d_i)
\end{equation}

Once trained, the network provides a fast approximation to the desired distribution that can be evaluated and sampled from efficiently, enabling standard Bayesian inference workflows.

\subsubsection{Information-Theoretic Perspective}

From an information-theoretic standpoint, \textbf{implicit inference preserves the complete information content} of the forward model without the approximations required for analytical tractability. Unlike summary statistics approaches that compress millions of measurements into a handful of correlation function bins, SBI can work directly with high-dimensional data vectors, capturing non-Gaussian features, higher-order correlations, and spatial patterns that traditional methods might miss.

However, this information preservation comes with a fundamental trade-off: \textbf{we exchange analytical certainty for empirical approximation}. While explicit methods provide exact answers (subject to their modeling assumptions), implicit methods introduce approximation errors through the neural network learning process. This necessitates careful validation procedures and uncertainty quantification techniques that we will explore later.

\subsubsection{Computational and Practical Considerations}

The transition to implicit inference fundamentally changes the computational bottleneck of Bayesian analysis. Instead of expensive MCMC chains that require likelihood evaluations at every step, the computational cost shifts to:

\begin{enumerate}
    \item \textbf{Simulation phase}: Generating training datasets $\{(\theta_i, d_i)\}_{i=1}^N$ through forward modeling
    \item \textbf{Training phase}: Optimizing neural network parameters through gradient-based methods
    \item \textbf{Inference phase}: Fast evaluation and sampling from the trained neural network
\end{enumerate}

This redistribution often proves advantageous because: (1) simulations can be parallelized efficiently across computational resources, (2) neural network training, while intensive, is a one-time cost that amortizes over multiple inference problems, and (3) once trained, the networks enable rapid exploration of different observational scenarios.

\bigskip

The shift to implicit inference represents more than a technical solution to computational constraints—it embodies a \textbf{fundamental reconceptualization of how we approach scientific inference in the era of complex simulations}. Rather than forcing realistic physics into analytically tractable approximations, we embrace the full complexity of our best physical models and use machine learning to bridge the gap between simulation capability and inference needs. This paradigm enables cosmology to fully leverage decades of investment in sophisticated simulation codes while maintaining the rigorous statistical framework of Bayesian analysis.

\section{Neural Density Estimation}

The core challenge in implicit inference is learning probability distributions from samples rather than analytical expressions. This section introduces the neural network machinery that enables SBI by approximating complex, high-dimensional probability densities from simulation data.

\subsection{The Density Estimation Problem}

In traditional statistics, we often work with known parametric families (e.g., Gaussian, exponential) where the functional form is specified and we only need to estimate parameters. For SBI, we face a fundamentally different challenge: **learning the shape of unknown probability distributions** from samples generated by black-box simulators.

Given samples $\{x_i\}_{i=1}^N$ drawn from an unknown distribution $p(x)$, density estimation seeks to construct an approximation $\hat{p}(x)$ that captures the underlying probability structure. In the SBI context, these samples come from running forward simulations: $x_i = \mathcal{S}(\theta_i, \epsilon_i)$ where $\theta_i$ are sampled parameters and $\epsilon_i$ represent simulation randomness.

\subsection{Neural Networks as Universal Approximators}

Neural networks provide the flexibility needed for this task through their **universal approximation capabilities**. A feedforward neural network with sufficient width can approximate any continuous function to arbitrary precision, making them natural candidates for density estimation.

The basic building block is a multilayer perceptron:
\begin{equation}
    f(x) = W_L \sigma(W_{L-1} \sigma(\cdots \sigma(W_1 x + b_1) \cdots) + b_{L-1}) + b_L
\end{equation}
where $W_\ell$ are weight matrices, $b_\ell$ are bias vectors, and $\sigma(\cdot)$ is a nonlinear activation function. The network learns optimal parameters through gradient-based optimization on training data.

For density estimation, the key insight is to use neural networks to parameterize probability distributions, then train them to maximize the likelihood of observed samples.

\subsection{Mixture Density Networks (MDNs)}

The conceptually simplest approach to neural density estimation uses **mixture models** where neural networks predict the parameters of a mixture of simple distributions. For a mixture of $K$ Gaussians:

\begin{equation}
    p(x|\theta) = \sum_{k=1}^K \pi_k(\theta) \mathcal{N}(x | \mu_k(\theta), \sigma_k(\theta))
\end{equation}

Here, neural networks $\pi_k(\cdot)$, $\mu_k(\cdot)$, and $\sigma_k(\cdot)$ output the mixture weights, means, and standard deviations respectively as functions of conditioning variables $\theta$. The networks are trained by maximizing the log-likelihood:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \log p(x_i|\theta_i)
\end{equation}

**Advantages**: Interpretable components, stable training, works well for unimodal or simple multimodal distributions.

**Limitations**: Limited flexibility for complex distributions, requires manual specification of mixture components $K$.

\subsection{Normalizing Flows}

For greater flexibility, **normalizing flows** learn invertible transformations between a simple base distribution (e.g., standard Gaussian) and the target distribution. The key idea is to apply a sequence of invertible functions $f = f_L \circ \cdots \circ f_1$ that transform samples $z \sim p_0(z)$ to $x = f(z)$.

The probability density follows from the change of variables formula:
\begin{equation}
    p(x) = p_0(z) \left| \det \frac{\partial f^{-1}}{\partial x} \right| = p_0(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right|
\end{equation}

Each transformation $f_i$ must be invertible with tractable Jacobian determinant. Common architectures include:
\begin{itemize}
    \item **Coupling flows**: Split dimensions and use neural networks to transform subsets
    \item **Autoregressive flows**: Transform dimensions sequentially using autoregressive dependencies
    \item **Continuous normalizing flows**: Use neural ODEs for infinite-depth transformations
\end{itemize}

**Advantages**: High expressiveness, exact likelihood computation, efficient sampling via forward pass.

**Limitations**: Architectural constraints for invertibility, computational cost of Jacobian determinants.

\subsection{Advanced Methods}

Recent developments have expanded the toolkit for neural density estimation:

\begin{itemize}
    \item **Score matching**: Instead of learning $p(x)$ directly, learn the score function $s(x) = \nabla_x \log p(x)$. The score characterizes the distribution and can be learned without normalization constants.
    
    \item **Diffusion models**: Model the gradual denoising process from pure noise to data samples. These have shown remarkable success in generative modeling and are increasingly used for density estimation.
    
    \item **Neural spline flows**: Use neural networks to parameterize monotonic spline functions, providing a good balance between flexibility and computational efficiency.
\end{itemize}

The choice of method depends on the specific requirements: MDNs for interpretability, normalizing flows for exact likelihoods, and advanced methods for maximum flexibility. In the next section, we explore how these density estimation techniques enable the three main approaches to SBI.

\section{From Density Estimation to Bayesian Inference}

\subsection{Sequential Neural Posterior Estimation (SNPE)}
Learn the posterior $p(\theta|x)$ directly using neural density estimation.

\subsection{Sequential Neural Likelihood Estimation (SNLE)}
Learn the likelihood $p(x|\theta)$ and use MCMC for posterior sampling.

\subsection{Neural Ratio Estimation (NRE)}
Learn likelihood ratios $r(x,\theta) = p(x|\theta) / p(x|\theta_0)$ for efficient inference.

\begin{remark}
The \texttt{sbi} package provides implementations of all these methods with a unified interface.
\end{remark}

\section{Dimensionality Reduction for SBI}

\subsection{Beyond Two-Point Functions}
Capturing non-Gaussian information in cosmological fields requires going beyond traditional summary statistics.

\subsection{Compression Strategies}
\begin{itemize}
    \item Scattering transforms
    \item Neural compression
    \item Information-maximizing summaries
\end{itemize}

Key principle: Preserve information relevant for inference while reducing dimensionality.

\section{Robust SBI in Practice}

\subsection{Key Considerations}
\begin{enumerate}
    \item \textbf{Frozen summarizers}: Keep feature extraction fixed during inference
    \item \textbf{Sample efficiency}: Active learning and sequential strategies
    \item \textbf{Ensemble methods}: Multiple networks for uncertainty quantification
    \item \textbf{Validation}: Simulation-based calibration and coverage tests
\end{enumerate}

\subsection{Common Pitfalls}
\begin{itemize}
    \item Distribution shift between simulations and real data
    \item Model misspecification
    \item Insufficient simulation fidelity
\end{itemize}

\section{Practical Resources}

\subsection{Software}
\begin{itemize}
    \item \texttt{sbi}: \url{https://github.com/mackelab/sbi}
    \item \texttt{lampe}: Likelihood-free AMortized Posterior Estimation
    \item \texttt{pydelfi}: Density Estimation Likelihood-Free Inference
\end{itemize}

\subsection{Key References}
\begin{itemize}
    \item Cranmer et al. (2020), ``The frontier of simulation-based inference''
    \item Papamakarios et al. (2019), ``Neural Density Estimation: A Review''
    \item Alsing \& Wandelt (2019), ``Generalized massive optimal data compression''
    \item Jeffrey et al. (2020), ``Likelihood-free inference with neural compression''
\end{itemize}

\end{document}