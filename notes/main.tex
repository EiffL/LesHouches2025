% =========================================================================
% SciPost LaTeX template
% Version 2024-07
%
% Submissions to SciPost Journals should make use of this template.
%
% INSTRUCTIONS: simply look for the `TODO:' tokens and adapt your file.
% ========================================================================

\documentclass{SciPost}

% Prevent all line breaks in inline equations.
\binoppenalty=10000
\relpenalty=10000

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage[bitstream-charter]{mathdesign}
\urlstyle{same}

% Fix \cal and \mathcal characters look (so it's not the same as \mathscr)
\DeclareSymbolFont{usualmathcal}{OMS}{cmsy}{m}{n}
\DeclareSymbolFontAlphabet{\mathcal}{usualmathcal}

\fancypagestyle{SPstyle}{
\fancyhf{}
\lhead{\colorbox{scipostblue}{\bf \color{white} ~SciPost Physics Lecture Notes }}
\rhead{{\bf \color{scipostdeepblue} ~Submission }}
\renewcommand{\headrulewidth}{1pt}
\fancyfoot[C]{\textbf{\thepage}}
}

\begin{document}

\pagestyle{SPstyle}

\begin{center}{\Large \textbf{\color{scipostdeepblue}{
  Simulation-Based Inference for Cosmology
}}}\end{center}

\begin{center}\textbf{
% Write the author list here. 
% Use (full) first name (+ middle name initials) + surname format.
% Separate subsequent authors by a comma, omit comma and use "and" for the last author.
% Mark the corresponding author(s) with a superscript symbol in this order
% \star, \dagger, \ddagger, \circ, \S, \P, \parallel, ...
François Lanusse\textsuperscript{1$\star$}
}\end{center}

\begin{center}
% Write all affiliations here.
% Format: institute, city, country
{\bf 1} Université Paris-Saclay, Université Paris Cité, CEA, CNRS, AIM, 91191, Gif-sur-Yvette, France
% Provide email address of corresponding author(s)
\\[\baselineskip]
$\star$ \href{mailto:francois.lanusse@cnrs.fr}{\small francois.lanusse@cnrs.fr}
\end{center}


\section*{\color{scipostdeepblue}{Abstract}}
\textbf{\boldmath{
Modern cosmological inference faces a fundamental challenge: extracting scientific insights from complex, high-dimensional datasets when traditional likelihood-based methods become intractable. This lecture introduces Simulation-Based Inference (SBI), a paradigm that replaces analytical likelihood evaluation with implicit distributions learned from forward simulations. We establish the theoretical foundations distinguishing explicit inference (where $p(d|\theta)$ can be evaluated) from implicit inference (where only samples from the joint distribution $p(\theta, d)$ are accessible through simulations). The lecture covers neural density estimation techniques—from Mixture Density Networks to Normalizing Flows—that enable learning complex probability distributions from simulation data. We demonstrate how these methods transform cosmological parameter estimation from summary statistic compression to full-field analysis, preserving complete information content while handling non-differentiable physics codes. Practical considerations for robust SBI implementation in cosmology are discussed, emphasizing validation strategies and common pitfalls when bridging simulations and observations.
}}

\vspace{\baselineskip}

%%%%%%%%%% BLOCK: Copyright information
% This block will be filled during the proof stage, and finilized just before publication.
% It exists here only as a placeholder, and should not be modified by authors.
\noindent\textcolor{white!90!black}{%
\fbox{\parbox{0.975\linewidth}{%
\textcolor{white!40!black}{\begin{tabular}{lr}%
  \begin{minipage}{0.6\textwidth}%
    {\small Copyright attribution to authors. \newline
    This work is a submission to SciPost Physics Lecture Notes. \newline
    License information to appear upon publication. \newline
    Publication information to appear upon publication.}
  \end{minipage} & \begin{minipage}{0.4\textwidth}
    {\small Received Date \newline Accepted Date \newline Published Date}%
  \end{minipage}
\end{tabular}}
}}
}
%%%%%%%%%% BLOCK: Copyright information


%%%%%%%%%% TODO: LINENO
% For convenience during refereeing we turn on line numbers:
\linenumbers
% You should run LaTeX twice in order for the line numbers to appear.
%%%%%%%%%% END TODO: LINENO

%%%%%%%%%% TODO: TOC 
% Guideline: if your paper is longer that 6 pages, include a TOC
% To remove the TOC, simply cut the following block
\vspace{10pt}
\noindent\rule{\textwidth}{1pt}
\tableofcontents
\noindent\rule{\textwidth}{1pt}
\vspace{10pt}
%%%%%%%%%% END TODO: TOC

\section*{Introduction}

Simulation-Based Inference (SBI) addresses a fundamental challenge in modern cosmological data analysis: performing Bayesian inference when the likelihood function $p(d|\theta)$ is intractable or computationally prohibitive to evaluate. Traditional approaches rely on analytical likelihood expressions or approximations that become inadequate for complex, high-dimensional data such as cosmic web fields, galaxy clustering, or gravitational lensing maps.

The key insight of SBI is to \textbf{replace explicit likelihood} evaluations required by traditional MCMC, \textbf{with an implicit likelihood learned from simulations}. In other words, rather than deriving the likelihood of the data $p(d|\theta)$ analytically, simulations can be used to generate synthetic training data that captures the relationship between parameters $\theta$ and observations $d$. Neural networks can then be used to approximate the relevant distributions (whether the posterior $p(\theta|d)$, likelihood $p(d|\theta)$, or likelihood ratios) directly from this simulation data.

This approach has proven particularly powerful in cosmology, where sophisticated N-body simulations and hydrodynamical codes can generate realistic synthetic observations, but the underlying physics is too complex for tractable likelihood expressions. SBI enables inference over high-dimensional parameter spaces using the full information content of complex datasets, moving beyond traditional summary statistics.

In these lecture notes, we will cover the connection between SBI and other forms of Bayesian inference, the theoretical foundations of neural density estimation powering SBI, and its practical applications in cosmological inference problems. We will also discuss the challenges and best practices for implementing SBI in real-world scenarios.


\section{MCMC, SBI, Full Field Inference... What is the difference?}

\subsection{Fundamental notations and definitions for Bayesian Inference}

Let us start by establishing the standard framework for Bayesian inference. Let $\theta \in \Theta \subseteq \mathbb{R}^p$ denote a vector of parameters of interest, and $d \in \mathcal{D}$ represent observed data. The fundamental quantities in Bayesian inference are:

\begin{itemize}
    \item \textbf{Prior distribution}: $p(\theta)$ encodes our beliefs about parameters before observing data
    \item \textbf{Likelihood function}: $p(d|\theta)$ specifies the probability of observing data $d$ given parameters $\theta$
    \item \textbf{Posterior distribution}: $p(\theta|d)$ represents our updated beliefs about parameters after observing data
    \item \textbf{Evidence (marginal likelihood)}: $p(d) = \int p(d|\theta)p(\theta) \, d\theta$ normalizes the posterior
\end{itemize}

Given these definitions, the famous Bayes' theorem connects these quantities according to:
\begin{equation}
    \boxed{p(\theta|d) = \frac{p(d|\theta)p(\theta)}{p(d)}}
\end{equation}

No matter the inference strategy, these definitions remain the same. SBI will differ from other Bayesian inference techniques only in the strategy used to estimate these distributions. 

\subsection{Explicit Inference}

\textbf{Explicit inference} forms the foundation of classical Bayesian computation and encompasses all approaches where the \textbf{likelihood function $p(d|\theta)$ can be evaluated point-wise} for any given combination of data $d$ and parameters $\theta$. This seemingly simple requirement enables the entire machinery of Markov Chain Monte Carlo (MCMC) algorithms that have dominated Bayesian inference for decades.

To understand why likelihood evaluation is crucial, consider how MCMC algorithms explore the posterior distribution. Whether using the Metropolis-Hastings algorithm, Gibbs sampling, or more sophisticated variants, these methods fundamentally rely on computing \textbf{likelihood ratios} to make accept-reject decisions. At each step, a proposal $\theta'$ is compared to the current state $\theta$ through the ratio:
\begin{equation}
    r = \frac{p(\theta'|d)}{p(\theta|d)} = \frac{p(d|\theta')p(\theta')}{p(d|\theta)p(\theta)}
\end{equation}
Without the ability to evaluate the likelihoods $p(d|\theta)$ and $p(d|\theta')$ at two different points in parameter space, this ratio cannot be computed, and the MCMC chain cannot advance. This fundamental dependency explains why explicit inference has been the dominant paradigm in computational statistics.

In practice, evaluating likelihoods for raw cosmological data presents a fundamental challenge. Consider a weak lensing survey observing shear measurements $\gamma_{i}$ for millions of galaxies across the sky. The likelihood $p(\{\gamma_i\}|\theta)$ for these raw measurements involves complex correlations due to cosmic structure, survey geometry, and measurement uncertainties—making analytical evaluation intractable. The same problem arises for galaxy positions, CMB temperature maps, or any high-dimensional cosmological dataset.

This tractability issue drove cosmologists to adopt \textbf{summary statistics} as an ingenious workaround. Instead of working with millions of individual measurements, we compress the data into a manageable set of statistics for which analytical likelihoods can be derived. The \textbf{two-point correlation function} became the workhorse of cosmological analysis precisely because its likelihood could be written down. For weak lensing, shear measurements from millions of galaxies are compressed into correlation functions $\xi_+(\theta)$ and $\xi_-(\theta)$, whose likelihood can be modeled as multivariate Gaussian:
\begin{equation}
    p(\xi_{\text{obs}}|\theta) = \mathcal{N}(\xi_{\text{obs}}; \xi_{\text{theory}}(\theta), C)
\end{equation}
where $\xi_{\text{theory}}(\theta)$ comes from analytical models (linear perturbation theory, halo models) and $C$ represents the covariance matrix. This Gaussian assumption is justified by the Central Limit Theorem when averaging over many independent modes, making the likelihood both tractable and well-understood.

This framework has enabled precise cosmological parameter constraints from Large Scale Structure surveys like DES, KiDS, and HSC. However, it comes at the cost a fundamental limitation: potential \textbf{information loss}. By compressing millions of measurements into a handful of correlation function bins, we may discard a significant amount of information contained in higher-order statistics, non-Gaussian features, and spatial correlations that cannot be captured by two-point functions, which are only theoretically optimal for the analysis of Gaussian fields.

To access the \textit{full information content} of a given survey while maintaining explicit inference, we must abandon summary statistics and instead model the measured fields in high dimension and in all their complexity, which is usualy referred to as \textbf{full-field inference}. This will require constructing complex models that represent the \textbf{complete data generation process} behind the observations, in other words, a forward simulation of the measured high-dimensional data.

Performing explicit inference over such models is possible but requires the introduction of a set of \textbf{latent variables} $z$ representing unobserved quantities like the true underlying cosmic fields. \textbf{The generative model becomes hierarchical} and can be expressed as:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    z|\theta &\sim p(z|\theta) \quad \text{(latent cosmic field)} \\
    d|z &\sim p(d|z) \quad \text{(observation process)}
\end{align}

The joint posterior over both parameters and latent variables is:
\begin{equation}
    p(\theta, z|d) \propto p(d|z)p(z|\theta)p(\theta)
\end{equation}

This framework remains \textbf{explicitly tractable} as long as each component can be evaluated: $p(d|z)$ models the observation process (e.g., Poisson noise, instrumental systematics), $p(z|\theta)$ represents the physical field generation process, and $p(\theta)$ encodes prior knowledge. Compared to summary-statistics-based explicit inference, we now need to infer the set of latent variable $z$ in addition to the comological parameters $\theta$ which can become very challening whe $z$ is high-dimensional. 

A paradigmatic example is the \textbf{BORG algorithm} (Bayesian Origin Reconstruction from Galaxies, see Florent's lecture in this school). BORG performs inference on the initial density field of the universe given observed galaxy positions. The hierarchical model structure is:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    \delta_0 &\sim \mathcal{N}(0, P(\theta)) \quad \text{(initial density field)} \\
    \delta(z) &= \mathcal{F}[\delta_0, \theta] \quad \text{(gravitational evolution)} \\
    n_g &= \text{Poisson}(\rho_g[\delta(z)]) \quad \text{(galaxy counts)}
\end{align}

Here, $\mathcal{F}$ represents forward modeling through Lagrangian Perturbation Theory (LPT), and $\rho_g$ is the biased galaxy density field. Crucially, even though $\mathcal{F}$ involves complex physics, the likelihood remains explicit because: (1) LPT provides an analytical (though approximate) mapping from initial to final conditions, (2) all intermediate variables ($\delta_0$, $\delta(z)$) are tracked explicitly, and (3) the observation model $p(n_g|\delta(z))$ is analytically tractable. BORG can simultaneously reconstruct the cosmic web and constrain cosmological parameters while utilizing the full spatial information in galaxy surveys.

From an information-theoretic perspective, explicit hierarchical inference represents the \textbf{theoretically optimal approach} to parameter estimation. By modeling the complete generative process without information-losing compression, these methods can achieve the theoretical minimum possible uncertainties (the Cramér-Rao bound). 

However, this theoretical optimality comes with important practical challenges. 

The first is \textbf{computational scaling}: the dimensionality of latent variable spaces grows rapidly with field resolution. A modest $128^3$ cosmic web simulation involves $\sim 2 \times 10^6$ latent variables, making standard MCMC inefficient. High-dimensional sampling requires sophisticated algorithms like Hamiltonian Monte Carlo (HMC) that can explore the posterior efficiently.

The second challenge involves \textbf{differentiability requirements}: algorithms like HMC require computing gradients of the log-posterior with respect to all variables, including the high-dimensional latent field. This necessitates that the forward model $p(z|\theta)$ be differentiable, a strong constraint that excludes many realistic simulation codes.

The third challenge is \textbf{model fidelity}: explicit models must make approximations to remain tractable. BORG's use of LPT, while differentiable, becomes inaccurate in highly nonlinear regimes. The fundamental tension between model complexity and computational tractability limits the realism achievable in explicit frameworks.

\bigskip

Wouldn't it be nice if we could perform Full-Field inference without running into these difficulties intrinsic to high-dimensional inference?

\subsection{Implicit Inference}

Having established that explicit inference reaches its limits when forward models become too complex to decompose into tractable probabilistic components, we now turn to scenarios where the likelihood $p(d|\theta)$ becomes fundamentally unevaluable. This transition from explicit to implicit inference represents a critical juncture in modern cosmological analysis.

We define \textbf{implicit inference} as any Bayesian inference approach where the likelihood function $p(d|\theta)$ cannot be evaluated directly, but where we can generate samples from the joint distribution $p(\theta, d)$ through forward simulations. In contrast to explicit methods that require point-wise likelihood evaluation, implicit methods work with \textbf{implicit distributions}: probability distributions that are defined through a generative process (such as running a simulation) rather than through an analytical expression that can be evaluated at arbitrary points.

This paradigm becomes necessary when the complexity of our models and simulations precludes explicit inference. This occurs in two main scenarios where the computational or mathematical requirements exceed what explicit methods can handle.

The first challenge arises from \textbf{high-dimensional inference problems}. When dealing with full-field inference in very high dimensions, even explicit hierarchical models like BORG face computational challenges. Sampling efficiently in spaces with millions of latent variables remains difficult, and the required computational resources can become prohibitive.

The second limitation involves \textbf{non-differentiable or intractable forward models}. Many realistic cosmological simulations cannot be easily decomposed into the tractable probabilistic components required for explicit inference. For instance, differentiable models of full hydrodynamical simulations with realistic baryonic feedback do not yet exist, making it impossible to apply gradient-based sampling methods or even evaluate the likelihood function analytically.

\subsubsection{The Fundamental Insight of Simulation-Based Inference}

Faced with these challenges, SBI recognizes a crucial asymmetry: \textbf{while we cannot evaluate} $p(d|\theta)$ \textbf{directly, we can generate samples from it effortlessly}. This observation leads to the central insight that transforms an intractable inference problem into a tractable machine learning problem.

The key recognition is that for any parameter configuration $\theta$, we can generate synthetic observations through the simple procedure:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(sample from prior)} \\
    d &= \mathcal{S}(\theta, \epsilon) \quad \text{(run forward simulation)}
\end{align}

This generates pairs $\{(\theta_i, d_i)\}_{i=1}^N$ that are exact samples from the joint distribution $p(\theta, d)$, despite our inability to evaluate the likelihood analytically. The forward simulation $\mathcal{S}$ can be arbitrarily complex—involving hydrodynamical physics, stochastic star formation, Monte Carlo radiative transfer, etc. as long as it can be executed.

Having access to samples from $p(\theta, d)$, all that remains it the ability to estimate conditional density functions like $p(d|\theta)$ or $p(\theta|d)$ given $\{(\theta_i, d_i)\}_{i=1}^N$.

\subsubsection{Neural Networks as Distribution Approximators}

This approach transforms Bayesian inference from an analytical derivation problem into a \textbf{supervised learning problem}. Neural networks serve as flexible function approximators that can learn complex, high-dimensional probability distributions from training data. The universal approximation theorem guarantees that sufficiently wide neural networks can approximate any continuous function to arbitrary precision, making them ideal candidates for this task.

The training process follows standard machine learning principles: we maximize the likelihood of observed simulation pairs under the neural network's parametric distribution. For example, to learn the posterior $p(\theta|d)$, we train a neural network $f_\phi(d)$ that outputs the parameters of a probability distribution over $\theta$, optimizing:
\begin{equation}
    \phi^* = \arg\max_\phi \sum_{i=1}^N \log p_\phi(\theta_i | d_i)
\end{equation}

Once trained, the network provides a fast approximation to the desired distribution that can be evaluated and sampled from efficiently, enabling standard Bayesian inference workflows.

\subsubsection{Information-Theoretic Perspective}

From an information-theoretic standpoint, \textbf{implicit inference preserves the complete information content} of the forward model without the approximations required for analytical tractability. Unlike summary statistics approaches that compress millions of measurements into a handful of correlation function bins, SBI can work directly with high-dimensional data vectors, capturing non-Gaussian features, higher-order correlations, and spatial patterns that traditional methods might miss.

However, this information preservation comes with a fundamental trade-off: \textbf{we exchange analytical certainty for empirical approximation}. While explicit methods provide exact answers (subject to their modeling assumptions), implicit methods introduce approximation errors through the neural network learning process. This necessitates careful validation procedures and uncertainty quantification techniques that we will explore later.

\subsubsection{Computational and Practical Considerations}

The transition to implicit inference fundamentally changes the computational bottleneck of Bayesian analysis. Instead of expensive MCMC chains that require likelihood evaluations at every step, the computational cost shifts to:

\begin{enumerate}
    \item \textbf{Simulation phase}: Generating training datasets $\{(\theta_i, d_i)\}_{i=1}^N$ through forward modeling
    \item \textbf{Training phase}: Optimizing neural network parameters through gradient-based methods
    \item \textbf{Inference phase}: Fast evaluation and sampling from the trained neural network
\end{enumerate}

This redistribution often proves advantageous because: (1) simulations can be parallelized efficiently across computational resources, (2) neural network training, while intensive, is a one-time cost that amortizes over multiple inference problems, and (3) once trained, the networks enable rapid exploration of different observational scenarios.

\bigskip

The shift to implicit inference represents more than a technical solution to computational constraints—it embodies a \textbf{fundamental reconceptualization of how we approach scientific inference in the era of complex simulations}. Rather than forcing realistic physics into analytically tractable approximations, we embrace the full complexity of our best physical models and use machine learning to bridge the gap between simulation capability and inference needs. This paradigm enables cosmology to fully leverage decades of investment in sophisticated simulation codes while maintaining the rigorous statistical framework of Bayesian analysis.

\section{Neural Density Estimation}

Having established that implicit inference transforms an intractable analytical problem into a supervised learning problem from simulation samples, we now confront the core technical challenge: \textbf{how do we learn probability distributions when we only have access to samples?} This question lies at the heart of making SBI practical, as our ability to perform Bayesian inference depends entirely on how well we can approximate the implicit distributions defined by our simulators.

\subsection{The Density Estimation Problem}

The transition from explicit to implicit inference fundamentally changes the nature of our computational task. In explicit inference, we derive analytical expressions for $p(d|\theta)$ based on physical models and statistical assumptions. In implicit inference, we face a radically different situation: we have samples $\{(\theta_i, d_i)\}_{i=1}^N$ generated by running our simulator, but \textbf{no functional form} for the underlying distributions.

This creates a profound challenge that traditional statistical methods were not designed to address. Classical density estimation techniques like kernel density estimation or histograms work well in low dimensions but suffer from the \textbf{curse of dimensionality} when applied to the high-dimensional parameter and data spaces typical in cosmology. A weak lensing convergence map might have millions of pixels, while cosmological parameter spaces often exceed 20 dimensions when including nuisance parameters. In such spaces, the number of samples required for traditional methods grows exponentially, quickly exceeding any practical computational budget.

The key insight that makes neural density estimation viable is recognizing that \textbf{cosmological distributions often lie on lower-dimensional manifolds} embedded in high-dimensional spaces. The cosmic web, for instance, exhibits characteristic patterns—filaments, voids, clusters—that represent a tiny fraction of all possible matter distributions. Similarly, the relationship between cosmological parameters and observables is highly structured, constrained by the physics of structure formation. Neural networks excel at learning these implicit lower-dimensional structures from data.

\subsection{Neural Networks as Universal Function Approximators}

The theoretical foundation for using neural networks in density estimation rests on the \textbf{universal approximation theorem}, which guarantees that feedforward neural networks with sufficient width can approximate any continuous function to arbitrary precision. This result, proven independently by Cybenko (1989) and Hornik (1991), provides the mathematical justification for believing that neural networks can learn the complex mappings required for SBI.

A standard feedforward neural network transforms an input $x$ through a sequence of linear transformations and nonlinear activations:
\begin{equation}
    f(x) = W_L \sigma(W_{L-1} \sigma(\cdots \sigma(W_1 x + b_1) \cdots) + b_{L-1}) + b_L
\end{equation}
where $W_\ell$ are weight matrices, $b_\ell$ are bias vectors, and $\sigma(\cdot)$ is a nonlinear activation function (typically ReLU, tanh, or sigmoid). The parameters $\{W_\ell, b_\ell\}$ are learned by minimizing a loss function through gradient-based optimization.

However, the universal approximation theorem alone is insufficient for density estimation. The crucial additional insight is that \textbf{we can use neural networks to parameterize probability distributions} rather than just deterministic functions. Instead of directly outputting density values, neural networks can output the parameters of probability distributions, which can then be trained using maximum likelihood estimation on our simulation samples.

This shift from function approximation to distribution parameterization opens the door to a rich variety of architectures, each designed to capture different aspects of complex probability distributions. The evolution of these methods—from simple mixture models to sophisticated normalizing flows—reflects the field's growing understanding of how to leverage neural network flexibility while maintaining the probabilistic rigor required for Bayesian inference.

\subsection{Mixture Density Networks (MDNs)}

The most natural starting point for neural density estimation builds on the familiar concept of mixture models. Rather than attempting to learn arbitrary probability distributions directly, we begin with the assumption that \textbf{complex distributions can be decomposed into combinations of simple, well-understood components}. This approach, known as Mixture Density Networks (MDNs), represents the earliest attempt to marry the flexibility of neural networks with the mathematical rigor of probability theory.

The core insight behind MDNs is that neural networks excel at learning complex conditional relationships between inputs and outputs. In the SBI context, if we have simulation pairs $\{(\theta_i, d_i)\}_{i=1}^N$, we can train a neural network to predict the parameters of a probability distribution over $d$ given $\theta$. The simplest choice is a mixture of Gaussians:

\begin{equation}
    p(d|\theta) = \sum_{k=1}^K \pi_k(\theta) \mathcal{N}(d | \mu_k(\theta), \Sigma_k(\theta))
\end{equation}

Here, three separate neural networks (or different output heads of a single network) learn to predict the mixture weights $\pi_k(\theta)$, means $\mu_k(\theta)$, and covariance matrices $\Sigma_k(\theta)$ as functions of the cosmological parameters $\theta$. The mixture weights must satisfy $\sum_k \pi_k(\theta) = 1$ and $\pi_k(\theta) \geq 0$, typically enforced through a softmax activation. The covariance matrices must be positive definite, often parameterized through Cholesky decomposition or diagonal approximations.

Training proceeds by maximizing the log-likelihood of the observed simulation data:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \log p(d_i|\theta_i) = \sum_{i=1}^N \log \sum_{k=1}^K \pi_k(\theta_i) \mathcal{N}(d_i | \mu_k(\theta_i), \Sigma_k(\theta_i))
\end{equation}

The power of this approach lies in its \textbf{conceptual simplicity and interpretability}. Each Gaussian component can potentially represent a different "mode" of the cosmic web—one component might capture void-dominated regions, another might represent filamentary structures, and a third could model cluster-rich areas. The mixture weights tell us the relative prevalence of each mode as a function of cosmological parameters.

However, this interpretability comes at a cost. The fundamental limitation of MDNs emerges from the \textbf{finite mixture assumption}: real cosmological distributions often exhibit complexity that cannot be adequately captured by a small number of Gaussian components. Consider the distribution of matter in a cosmic web simulation—the intricate network of filaments, the complex shapes of voids, the hierarchical structure of dark matter halos. These features require either an impractically large number of mixture components or acceptance of significant approximation errors.

Moreover, the choice of $K$ (the number of mixture components) introduces a \textbf{model selection problem} that lacks a principled solution. Too few components lead to underfitting, while too many can cause overfitting and computational instability. This tension between model complexity and tractability motivated the development of more flexible approaches that could adapt their complexity to the data rather than requiring it to be specified a priori.

\subsection{Normalizing Flows}

The limitations of MDNs—their reliance on fixed architectural choices and finite mixture assumptions—motivated a more fundamental approach to neural density estimation. Rather than constraining ourselves to specific parametric families, what if we could \textbf{learn arbitrary transformations} that map complex distributions to simple ones? This insight gave birth to normalizing flows, a revolutionary approach that transforms the density estimation problem into learning invertible mappings.

The conceptual breakthrough underlying normalizing flows rests on a simple mathematical observation: if we can construct an invertible transformation $f$ that maps samples from a complex distribution $p(x)$ to samples from a simple base distribution $p_0(z)$ (typically standard Gaussian), then we can compute exact densities using the change of variables formula from probability theory. This transforms the intractable problem of learning arbitrary densities into the more manageable problem of learning invertible functions.

Consider a sequence of invertible transformations $f = f_L \circ f_{L-1} \circ \cdots \circ f_1$ that maps samples $z \sim p_0(z)$ from a simple base distribution to samples $x = f(z)$ from our target distribution. The probability density of $x$ follows directly from the change of variables theorem:

\begin{equation}
    p(x) = p_0(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right|
\end{equation}

This elegant mathematical relationship provides the foundation for training normalizing flows. Given simulation data $\{x_i\}_{i=1}^N$, we can train the parameters of the transformations $\{f_\ell\}$ by maximizing the log-likelihood:

\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \log p(x_i) = \sum_{i=1}^N \left[ \log p_0(f^{-1}(x_i)) + \log \left| \det \frac{\partial f^{-1}}{\partial x} \right|_{x=x_i} \right]
\end{equation}

The power of this approach lies in its theoretical generality: \textbf{any invertible transformation can, in principle, be learned}. This means that normalizing flows can capture arbitrarily complex distributions, including those with intricate correlation structures, multiple modes, heavy tails, and sharp boundaries—all features commonly found in cosmological simulations.

However, the requirement for invertibility with tractable Jacobian determinants imposes significant architectural constraints. The transformation must be designed such that: (1) the inverse $f^{-1}$ can be computed efficiently, (2) the Jacobian determinant can be evaluated without excessive computational cost, and (3) the function has sufficient expressiveness to capture complex distributions.

These constraints led to several ingenious architectural solutions. \textbf{Coupling flows} address the invertibility requirement by splitting the input dimensions and using one subset to transform the other through additive or affine transformations. \textbf{Autoregressive flows} ensure invertibility by making each output dimension depend only on previous dimensions, allowing sequential computation of both forward and inverse transformations. \textbf{Continuous normalizing flows} use neural ordinary differential equations (ODEs) to define infinitely deep transformations with tractable Jacobians.

The success of normalizing flows in cosmological applications stems from their ability to capture the \textbf{non-linear correlations and complex geometries} inherent in cosmic structure formation. For instance, the relationship between cosmological parameters and weak lensing maps involves intricate dependencies mediated by the non-linear growth of structure, baryonic feedback, and projection effects. Traditional methods like MDNs struggle with such complexity, while normalizing flows can learn these relationships directly from simulation data.

Yet this flexibility comes with computational costs. The requirement to evaluate Jacobian determinants—especially for high-dimensional problems—can make normalizing flows significantly more expensive than simpler alternatives. Moreover, ensuring numerical stability during training requires careful architectural design and regularization techniques. These considerations led researchers to explore even more flexible approaches that could capture distributional complexity without the rigid constraints imposed by invertibility requirements.

\subsection{Beyond Explicit Density Models}

While MDNs and normalizing flows represent major advances in neural density estimation, both approaches share a fundamental assumption: that we need to explicitly model probability densities $p(x)$. This assumption, though natural, is not always necessary for statistical inference. Recent developments have challenged this paradigm by recognizing that \textbf{many inference tasks can be performed without explicit density evaluation}, leading to methods that sidestep some of the most challenging aspects of density modeling.

The limitations that motivated these advanced approaches are subtle but significant. MDNs, despite their interpretability, struggle with high-dimensional distributions and require careful tuning of mixture complexity. Normalizing flows, while theoretically powerful, impose architectural constraints that can limit their practical applicability and make them computationally expensive for very high-dimensional problems. Moreover, both approaches require careful handling of numerical stability issues when dealing with distributions that have sharp boundaries or extreme aspect ratios—situations commonly encountered in cosmological simulations.

\textbf{Score matching} represents a radical departure from explicit density modeling by focusing on the score function $s(x) = \nabla_x \log p(x)$ rather than the density itself. The key insight is that the score function characterizes a probability distribution uniquely, but can often be learned more easily than the density. Crucially, the score function avoids the normalization constant that makes density estimation challenging, since $\nabla_x \log p(x) = \nabla_x \log \tilde{p}(x)$ for any unnormalized density $\tilde{p}(x)$. Training proceeds by matching the learned score function to the true score, which can be estimated from data without knowing the underlying density.

\textbf{Diffusion models} take this approach even further by modeling the entire generative process through a gradual denoising procedure. Instead of learning a single transformation from noise to data (as in normalizing flows), diffusion models learn to reverse a forward noising process that gradually transforms data samples into pure noise. This approach has shown remarkable success in generative modeling applications and is increasingly being adapted for density estimation tasks in scientific applications.

\textbf{Neural spline flows} represent a hybrid approach that maintains the invertibility requirements of normalizing flows while using more flexible and computationally efficient transformations. By parameterizing transformations as monotonic spline functions rather than general neural networks, these methods achieve a favorable balance between expressiveness and computational cost.

These advanced methods reflect the field's growing sophistication in recognizing that \textbf{different inference problems may require different distributional representations}. For applications requiring exact likelihood evaluation (such as model comparison), normalizing flows remain the method of choice. For scenarios where interpretability is paramount, MDNs offer unmatched clarity. For complex, high-dimensional problems where computational efficiency is critical, score-based methods may provide the best solution.

The evolution from MDNs to normalizing flows to score-based methods illustrates a broader pattern in machine learning: the progression from constrained, interpretable models to flexible, general-purpose approaches, and finally to specialized methods tailored for specific problem characteristics. In the context of SBI, this evolution has opened up new possibilities for tackling previously intractable inference problems in cosmology and other scientific domains.

\section{From Density Estimation to Bayesian Inference}

\subsection{Sequential Neural Posterior Estimation (SNPE)}
Learn the posterior $p(\theta|x)$ directly using neural density estimation.

\subsection{Sequential Neural Likelihood Estimation (SNLE)}
Learn the likelihood $p(x|\theta)$ and use MCMC for posterior sampling.

\subsection{Neural Ratio Estimation (NRE)}
Learn likelihood ratios $r(x,\theta) = p(x|\theta) / p(x|\theta_0)$ for efficient inference.

\section{Dimensionality Reduction for SBI}

\subsection{Beyond Two-Point Functions}
Capturing non-Gaussian information in cosmological fields requires going beyond traditional summary statistics.

\subsection{Compression Strategies}
\begin{itemize}
    \item Scattering transforms
    \item Neural compression
    \item Information-maximizing summaries
\end{itemize}

Key principle: Preserve information relevant for inference while reducing dimensionality.

\section{Robust SBI in Practice}

\subsection{Key Considerations}
\begin{enumerate}
    \item \textbf{Frozen summarizers}: Keep feature extraction fixed during inference
    \item \textbf{Sample efficiency}: Active learning and sequential strategies
    \item \textbf{Ensemble methods}: Multiple networks for uncertainty quantification
    \item \textbf{Validation}: Simulation-based calibration and coverage tests
\end{enumerate}

\subsection{Common Pitfalls}
\begin{itemize}
    \item Distribution shift between simulations and real data
    \item Model misspecification
    \item Insufficient simulation fidelity
\end{itemize}

\section{Practical Resources}

\subsection{Software}
\begin{itemize}
    \item \texttt{sbi}: \url{https://github.com/mackelab/sbi}
    \item \texttt{lampe}: Likelihood-free AMortized Posterior Estimation
    \item \texttt{pydelfi}: Density Estimation Likelihood-Free Inference
\end{itemize}







\section{Introduction}
\label{sec:intro}
% TODO: write your article here.
The stage is yours. Write your article here.
The bulk of the paper should be clearly divided into sections with short descriptive titles, including an introduction and a conclusion.



\section{A Section}
Use sections to structure your article's presentation.

Equations should be centered; multi-line equations should be aligned.
\begin{equation}
H = \sum_{j=1}^N \left[J (S^x_j S^x_{j+1} + S^y_j S^y_{j+1} + \Delta S^z_j S^z_{j+1}) - h S^z_j \right].
\end{equation}

In the list of references, cited papers \cite{1931_Bethe_ZP_71} should include authors, title, journal reference (journal name, volume number (in bold), start page) and most importantly a DOI link. For a preprint \cite{arXiv:1108.2700}, please include authors, title (please ensure proper capitalization) and arXiv link. If you use BiBTeX with our style file, the right format will be automatically implemented.

All equations and references should be hyperlinked to ensure ease of navigation. This also holds for [sub]sections: readers should be able to easily jump to Section \ref{sec:another}.

\section{Another Section}
\label{sec:another}
There is no strict length limitation, but the authors are strongly encouraged to keep contents to the strict minimum necessary for peers to reproduce the research described in the paper.

\subsection{A first subsection}
You are free to use dividers as you see fit.
\subsection{A note about figures}
Figures should only occupy the stricly necessary space, in any case individually fitting on a single page. Each figure item should be appropriately labeled and accompanied by a descriptive caption. {\bf SciPost does not accept creative or promotional figures or artist's impressions}; on the other hand, technical drawings and scientifically accurate representations are encouraged.


\section{Conclusion}
You must include a conclusion.

\section*{Acknowledgements}
Acknowledgements should follow immediately after the conclusion.

% TODO: include author contributions
\paragraph{Author contributions}
This is optional. If desired, contributions should be succinctly described in a single short paragraph, using author initials.

% TODO: include funding information
\paragraph{Funding information}
Authors are required to provide funding information, including relevant agencies and grant numbers with linked author's initials. Correctly-provided data will be linked to funders listed in the \href{https://www.crossref.org/services/funder-registry/}{\sf Fundref registry}.


\begin{appendix}
\numberwithin{equation}{section}

\section{First appendix}
Add material which is better left outside the main text in a series of Appendices labeled by capital letters.

\section{About references}
Your references should start with the comma-separated author list (initials + last name), the publication title in italics, the journal reference with volume in bold, start page number, publication year in parenthesis, completed by the DOI link (linking must be implemented before publication). If using BiBTeX, please use the style files provided  on \url{https://scipost.org/submissions/author_guidelines}. If you are using our LaTeX template, simply add
\begin{verbatim}
\bibliography{your_bibtex_file}
\end{verbatim}
at the end of your document. If you are not using our LaTeX template, please still use our bibstyle as
\begin{verbatim}
\bibliographystyle{SciPost_bibstyle}
\end{verbatim}
in order to simplify the production of your paper.
\end{appendix}


%%%%%%%%% END TODO: CONTENTS


%%%%%%%%%% TODO: BIBLIOGRAPHY
% Provide your bibliography here. You have two options:

%%% FIRST OPTION
% Write your entries here directly, following the example below, including:
% Author(s), Title, Journal Ref. with year in parentheses at the end, followed by the DOI number.

%\begin{thebibliography}{99}
%\bibitem{1931_Bethe_ZP_71} H. A. Bethe, {\it Zur Theorie der Metalle. i. Eigenwerte und Eigenfunktionen der linearen Atomkette}, Zeit. f{\"u}r Phys. {\bf 71}, 205 (1931), \doi{10.1007\%2FBF01341708}.
%\bibitem{arXiv:1108.2700} P. Ginsparg, {\it It was twenty years ago today... }, \url{http://arxiv.org/abs/1108.2700}.
%\end{thebibliography}

%%% SECOND OPTION
% Use your bibtex library, formatted by the SciPost style file.
\bibliography{SciPost_Example_BiBTeX_File.bib}

%%%%%%%%%% END TODO: BIBLIOGRAPHY


\end{document}
