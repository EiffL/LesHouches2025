% =========================================================================
% SciPost LaTeX template
% Version 2024-07
%
% Submissions to SciPost Journals should make use of this template.
%
% INSTRUCTIONS: simply look for the `TODO:' tokens and adapt your file.
% ========================================================================

\documentclass{SciPost}

% Prevent all line breaks in inline equations.
\binoppenalty=10000
\relpenalty=10000

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage[bitstream-charter]{mathdesign}
\urlstyle{same}

% Fix \cal and \mathcal characters look (so it's not the same as \mathscr)
\DeclareSymbolFont{usualmathcal}{OMS}{cmsy}{m}{n}
\DeclareSymbolFontAlphabet{\mathcal}{usualmathcal}

\fancypagestyle{SPstyle}{
\fancyhf{}
\lhead{\colorbox{scipostblue}{\bf \color{white} ~SciPost Physics Lecture Notes }}
\rhead{{\bf \color{scipostdeepblue} ~Submission }}
\renewcommand{\headrulewidth}{1pt}
\fancyfoot[C]{\textbf{\thepage}}
}

\begin{document}

\pagestyle{SPstyle}

\begin{center}{\Large \textbf{\color{scipostdeepblue}{
  Simulation-Based Inference for Cosmology
}}}\end{center}

\begin{center}\textbf{
% Write the author list here. 
% Use (full) first name (+ middle name initials) + surname format.
% Separate subsequent authors by a comma, omit comma and use "and" for the last author.
% Mark the corresponding author(s) with a superscript symbol in this order
% \star, \dagger, \ddagger, \circ, \S, \P, \parallel, ...
François Lanusse\textsuperscript{1$\star$}
}\end{center}

\begin{center}
% Write all affiliations here.
% Format: institute, city, country
{\bf 1} Université Paris-Saclay, Université Paris Cité, CEA, CNRS, AIM, 91191, Gif-sur-Yvette, France
% Provide email address of corresponding author(s)
\\[\baselineskip]
$\star$ \href{mailto:francois.lanusse@cnrs.fr}{\small francois.lanusse@cnrs.fr}
\end{center}


\section*{\color{scipostdeepblue}{Abstract}}
\textbf{\boldmath{
Modern cosmological datasets increasingly challenge traditional likelihood-based Bayesian inference, limiting our ability to extract full information content from surveys. This lecture covers Simulation-Based Inference (SBI), which replaces tractable likelihood evaluations with distributions learned from black-box simulations. We cover neural density estimation techniques, from Mixture Density Networks to Normalizing Flows, for learning complex probability distributions from simulation data. These methods enable full-field cosmological parameter estimation while handling non-differentiable physics codes, moving beyond summary statistic compression to preserve complete information content. Practical implementation considerations are discussed, including validation strategies and common pitfalls.
}}

\vspace{\baselineskip}

%%%%%%%%%% BLOCK: Copyright information
% This block will be filled during the proof stage, and finilized just before publication.
% It exists here only as a placeholder, and should not be modified by authors.
\noindent\textcolor{white!90!black}{%
\fbox{\parbox{0.975\linewidth}{%
\textcolor{white!40!black}{\begin{tabular}{lr}%
  \begin{minipage}{0.6\textwidth}%
    {\small Copyright attribution to authors. \newline
    This work is a submission to SciPost Physics Lecture Notes. \newline
    License information to appear upon publication. \newline
    Publication information to appear upon publication.}
  \end{minipage} & \begin{minipage}{0.4\textwidth}
    {\small Received Date \newline Accepted Date \newline Published Date}%
  \end{minipage}
\end{tabular}}
}}
}
%%%%%%%%%% BLOCK: Copyright information


%%%%%%%%%% TODO: LINENO
% For convenience during refereeing we turn on line numbers:
\linenumbers
% You should run LaTeX twice in order for the line numbers to appear.
%%%%%%%%%% END TODO: LINENO

%%%%%%%%%% TODO: TOC 
% Guideline: if your paper is longer that 6 pages, include a TOC
% To remove the TOC, simply cut the following block
\vspace{10pt}
\noindent\rule{\textwidth}{1pt}
\tableofcontents
\noindent\rule{\textwidth}{1pt}
\vspace{10pt}
%%%%%%%%%% END TODO: TOC

\section{Introduction}

Simulation-Based Inference (SBI) addresses a fundamental challenge in modern cosmological data analysis: performing Bayesian inference when the likelihood function $p(d|\theta)$ is intractable or computationally prohibitive to evaluate. Traditional approaches rely on analytical likelihood expressions or approximations that become inadequate for complex, high-dimensional data such as cosmic web fields, galaxy clustering, or gravitational lensing maps.

The key insight of SBI is to \textbf{replace explicit likelihood} evaluations required by traditional MCMC, \textbf{with an implicit likelihood learned from simulations}. In other words, rather than deriving the likelihood of the data $p(d|\theta)$ analytically, simulations can be used to generate synthetic training data that captures the relationship between parameters $\theta$ and observations $d$. Neural networks can then be used to approximate the relevant distributions (whether the posterior $p(\theta|d)$, likelihood $p(d|\theta)$, or likelihood ratios) directly from this simulation data.

This approach has proven particularly powerful in cosmology, where sophisticated N-body simulations and hydrodynamical codes can generate realistic synthetic observations, but the underlying physics is too complex for tractable likelihood expressions. SBI enables inference over high-dimensional parameter spaces using the full information content of complex datasets, moving beyond traditional summary statistics.

In these lecture notes, we will cover the connection between SBI and other forms of Bayesian inference, the theoretical foundations of neural density estimation powering SBI, and its practical applications in cosmological inference problems. We will also discuss the challenges and best practices for implementing SBI in real-world scenarios.

\section{MCMC, SBI, Full Field Inference... What is the difference?}

\subsection{Fundamental notations and definitions for Bayesian Inference}

Let us start by establishing the standard framework for Bayesian inference. Let $\theta \in \Theta \subseteq \mathbb{R}^p$ denote a vector of parameters of interest, and $d \in \mathcal{D}$ represent observed data. The fundamental quantities in Bayesian inference are:

\begin{itemize}
    \item \textbf{Prior distribution}: $p(\theta)$ encodes our beliefs about parameters before observing data
    \item \textbf{Likelihood function}: $p(d|\theta)$ specifies the probability of observing data $d$ given parameters $\theta$
    \item \textbf{Posterior distribution}: $p(\theta|d)$ represents our updated beliefs about parameters after observing data
    \item \textbf{Evidence (marginal likelihood)}: $p(d) = \int p(d|\theta)p(\theta) \, d\theta$ normalizes the posterior
\end{itemize}

Given these definitions, the famous Bayes' theorem connects these quantities according to:
\begin{equation}
    \boxed{p(\theta|d) = \frac{p(d|\theta)p(\theta)}{p(d)}}
\end{equation}

No matter the inference strategy, these definitions remain the same. SBI will differ from other Bayesian inference techniques only in the strategy used to estimate these distributions. 

\subsection{Explicit Inference}

\textbf{Explicit inference} forms the foundation of classical Bayesian computation and encompasses all approaches where the \textbf{likelihood function $p(d|\theta)$ can be evaluated point-wise} for any given combination of data $d$ and parameters $\theta$. This seemingly simple requirement enables the entire machinery of Markov Chain Monte Carlo (MCMC) algorithms that have dominated Bayesian inference for decades.

To understand why likelihood evaluation is crucial, consider how MCMC algorithms explore the posterior distribution. Whether using the Metropolis-Hastings algorithm, Gibbs sampling, or more sophisticated variants, these methods fundamentally rely on computing \textbf{likelihood ratios} to make accept-reject decisions. At each step, a proposal $\theta'$ is compared to the current state $\theta$ through the ratio:
\begin{equation}
    r = \frac{p(\theta'|d)}{p(\theta|d)} = \frac{p(d|\theta')p(\theta')}{p(d|\theta)p(\theta)}
\end{equation}
Without the ability to evaluate the likelihoods $p(d|\theta)$ and $p(d|\theta')$ at two different points in parameter space, this ratio cannot be computed, and the MCMC chain cannot advance. This fundamental dependency explains why explicit inference has been the dominant paradigm in computational statistics.

In practice, evaluating likelihoods for raw cosmological data presents a fundamental challenge. Consider a weak lensing survey observing shear measurements $\gamma_{i}$ for millions of galaxies across the sky. The likelihood $p(\{\gamma_i\}|\theta)$ for these raw measurements involves complex correlations due to cosmic structure, survey geometry, and measurement uncertainties—making analytical evaluation intractable. The same problem arises for galaxy positions, CMB temperature maps, or any high-dimensional cosmological dataset.

This tractability issue drove cosmologists to adopt \textbf{summary statistics} as an ingenious workaround. Instead of working with millions of individual measurements, we compress the data into a manageable set of statistics for which analytical likelihoods can be derived. The \textbf{two-point correlation function} became the workhorse of cosmological analysis precisely because its likelihood could be written down. For weak lensing, shear measurements from millions of galaxies are compressed into correlation functions $\xi_+(\theta)$ and $\xi_-(\theta)$, whose likelihood can be modeled as multivariate Gaussian:
\begin{equation}
    p(\xi_{\text{obs}}|\theta) = \mathcal{N}(\xi_{\text{obs}}; \xi_{\text{theory}}(\theta), C)
\end{equation}
where $\xi_{\text{theory}}(\theta)$ comes from analytical models (linear perturbation theory, halo models) and $C$ represents the covariance matrix. This Gaussian assumption is justified by the Central Limit Theorem when averaging over many independent modes, making the likelihood both tractable and well-understood.

This framework has enabled precise cosmological parameter constraints from Large Scale Structure surveys like DES, KiDS, and HSC. However, it comes at the cost a fundamental limitation: potential \textbf{information loss}. By compressing millions of measurements into a handful of correlation function bins, we may discard a significant amount of information contained in higher-order statistics, non-Gaussian features, and spatial correlations that cannot be captured by two-point functions, which are only theoretically optimal for the analysis of Gaussian fields.

To access the \textit{full information content} of a given survey while maintaining explicit inference, we must abandon summary statistics and instead model the measured fields in high dimension and in all their complexity, which is usualy referred to as \textbf{full-field inference}. This will require constructing complex models that represent the \textbf{complete data generation process} behind the observations, in other words, a forward simulation of the measured high-dimensional data.

Performing explicit inference over such models is possible but requires the introduction of a set of \textbf{latent variables} $z$ representing unobserved quantities like the true underlying cosmic fields. \textbf{The generative model becomes hierarchical} and can be expressed as:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    z|\theta &\sim p(z|\theta) \quad \text{(latent cosmic field)} \\
    d|z &\sim p(d|z) \quad \text{(observation process)}
\end{align}

The joint posterior over both parameters and latent variables is:
\begin{equation}
    p(\theta, z|d) \propto p(d|z)p(z|\theta)p(\theta)
\end{equation}

This framework remains \textbf{explicitly tractable} as long as each component can be evaluated: $p(d|z)$ models the observation process (e.g., Poisson noise, instrumental systematics), $p(z|\theta)$ represents the physical field generation process, and $p(\theta)$ encodes prior knowledge. Compared to summary-statistics-based explicit inference, we now need to infer the set of latent variable $z$ in addition to the comological parameters $\theta$ which can become very challening whe $z$ is high-dimensional. 

A paradigmatic example is the \textbf{BORG algorithm} (Bayesian Origin Reconstruction from Galaxies, see Florent's lecture in this school). BORG performs inference on the initial density field of the universe given observed galaxy positions. The hierarchical model structure is:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    \delta_0 &\sim \mathcal{N}(0, P(\theta)) \quad \text{(initial density field)} \\
    \delta(z) &= \mathcal{F}[\delta_0, \theta] \quad \text{(gravitational evolution)} \\
    n_g &= \text{Poisson}(\rho_g[\delta(z)]) \quad \text{(galaxy counts)}
\end{align}

Here, $\mathcal{F}$ represents forward modeling through Lagrangian Perturbation Theory (LPT), and $\rho_g$ is the biased galaxy density field. Crucially, even though $\mathcal{F}$ involves complex physics, the likelihood remains explicit because: (1) LPT provides an analytical (though approximate) mapping from initial to final conditions, (2) all intermediate variables ($\delta_0$, $\delta(z)$) are tracked explicitly, and (3) the observation model $p(n_g|\delta(z))$ is analytically tractable. BORG can simultaneously reconstruct the cosmic web and constrain cosmological parameters while utilizing the full spatial information in galaxy surveys.

From an information-theoretic perspective, explicit hierarchical inference represents the \textbf{theoretically optimal approach} to parameter estimation. By modeling the complete generative process without information-losing compression, these methods can achieve the theoretical minimum possible uncertainties (the Cramér-Rao bound). 

However, this theoretical optimality comes with important practical challenges. 

The first is \textbf{computational scaling}: the dimensionality of latent variable spaces grows rapidly with field resolution. A modest $128^3$ cosmic web simulation involves $\sim 2 \times 10^6$ latent variables, making standard MCMC inefficient. High-dimensional sampling requires sophisticated algorithms like Hamiltonian Monte Carlo (HMC) that can explore the posterior efficiently.

The second challenge involves \textbf{differentiability requirements}: algorithms like HMC require computing gradients of the log-posterior with respect to all variables, including the high-dimensional latent field. This necessitates that the forward model $p(z|\theta)$ be differentiable, a strong constraint that excludes many realistic simulation codes.

The third challenge is \textbf{model fidelity}: explicit models must make approximations to remain tractable. BORG's use of LPT, while differentiable, becomes inaccurate in highly nonlinear regimes. The fundamental tension between model complexity and computational tractability limits the realism achievable in explicit frameworks.

\bigskip

Wouldn't it be nice if we could perform Full-Field inference without running into these difficulties intrinsic to high-dimensional inference?

\subsection{Implicit Inference}

Having established that explicit inference reaches its limits when forward models become too complex to decompose into tractable probabilistic components, we now turn to scenarios where the likelihood $p(d|\theta)$ becomes fundamentally unevaluable. This transition from explicit to implicit inference represents a critical juncture in modern cosmological analysis.

We define \textbf{implicit inference} as any Bayesian inference approach where the likelihood function $p(d|\theta)$ cannot be evaluated directly, but where we can generate samples from the joint distribution $p(\theta, d)$ through forward simulations. In contrast to explicit methods that require point-wise likelihood evaluation, implicit methods work with \textbf{implicit distributions}: probability distributions that are defined through a generative process (such as running a simulation) rather than through an analytical expression that can be evaluated at arbitrary points.

This paradigm becomes necessary when the complexity of our models and simulations precludes explicit inference. This occurs in two main scenarios where the computational or mathematical requirements exceed what explicit methods can handle.

The first challenge arises from \textbf{high-dimensional inference problems}. When dealing with full-field inference in very high dimensions, even explicit hierarchical models like BORG face computational challenges. Sampling efficiently in spaces with millions of latent variables remains difficult, and the required computational resources can become prohibitive.

The second limitation involves \textbf{non-differentiable or intractable forward models}. Many realistic cosmological simulations cannot be easily decomposed into the tractable probabilistic components required for explicit inference. For instance, differentiable models of full hydrodynamical simulations with realistic baryonic feedback do not yet exist, making it impossible to apply gradient-based sampling methods or even evaluate the likelihood function analytically.

\subsubsection{The Fundamental Insight of Simulation-Based Inference}

Faced with these challenges, SBI recognizes a crucial asymmetry: \textbf{while we cannot evaluate} $p(d|\theta)$ \textbf{directly, we can generate samples from it effortlessly}. This observation leads to the central insight that transforms an intractable inference problem into a tractable machine learning problem.

The key recognition is that for any parameter configuration $\theta$, we can generate synthetic observations through the simple procedure:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(sample from prior)} \\
    d &= \mathcal{S}(\theta, \epsilon) \quad \text{(run forward simulation)}
\end{align}

This generates pairs $\{(\theta_i, d_i)\}_{i=1}^N$ that are exact samples from the joint distribution $p(\theta, d)$, despite our inability to evaluate the likelihood analytically. The forward simulation $\mathcal{S}$ can be arbitrarily complex—involving hydrodynamical physics, stochastic star formation, Monte Carlo radiative transfer, etc. as long as it can be executed.

Having access to samples from $p(\theta, d)$, all that remains it the ability to estimate conditional density functions like $p(d|\theta)$ or $p(\theta|d)$ given $\{(\theta_i, d_i)\}_{i=1}^N$.

\subsubsection{Neural Networks as Distribution Approximators}

This approach transforms Bayesian inference from an analytical derivation problem into a \textbf{supervised learning problem}. Neural networks serve as flexible function approximators that can learn complex, high-dimensional probability distributions from training data. The universal approximation theorem guarantees that sufficiently wide neural networks can approximate any continuous function to arbitrary precision, making them ideal candidates for this task.

The training process follows standard machine learning principles: we maximize the likelihood of observed simulation pairs under the neural network's parametric distribution. For example, to learn the posterior $p(\theta|d)$, we train a neural network $f_\phi(d)$ that outputs the parameters of a probability distribution over $\theta$, optimizing:
\begin{equation}
    \phi^* = \arg\max_\phi \sum_{i=1}^N \log p_\phi(\theta_i | d_i)
\end{equation}

Once trained, the network provides a fast approximation to the desired distribution that can be evaluated and sampled from efficiently, enabling standard Bayesian inference workflows.

\subsubsection{Information-Theoretic Perspective}

From an information-theoretic standpoint, \textbf{implicit inference preserves the complete information content} of the forward model without the approximations required for analytical tractability. Unlike summary statistics approaches that compress millions of measurements into a handful of correlation function bins, SBI can work directly with high-dimensional data vectors, capturing non-Gaussian features, higher-order correlations, and spatial patterns that traditional methods might miss.

However, this information preservation comes with a fundamental trade-off: \textbf{we exchange analytical certainty for empirical approximation}. While explicit methods provide exact answers (subject to their modeling assumptions), implicit methods introduce approximation errors through the neural network learning process. This necessitates careful validation procedures and uncertainty quantification techniques that we will explore later.


\bigskip

The shift to implicit inference represents more than a technical solution to computational constraints—it embodies a \textbf{fundamental reconceptualization of how we approach scientific inference in the era of complex simulations}. Rather than forcing realistic physics into analytically tractable approximations, we embrace the full complexity of our best physical models and use machine learning to bridge the gap between simulation capability and inference needs. This paradigm enables cosmology to fully leverage decades of investment in sophisticated simulation codes while maintaining the rigorous statistical framework of Bayesian analysis.

\section{Neural Density Estimation}

Having established that implicit inference transforms an intractable analytical problem into a supervised learning problem from simulation samples, we now confront the core technical challenge: \textbf{how do we learn probability distributions when we only have access to samples?} This question lies at the heart of making SBI practical, as our ability to perform Bayesian inference depends entirely on how well we can approximate the implicit distributions defined by our simulators.

\subsection{The Density Estimation Problem}

The transition from explicit to implicit inference fundamentally changes the nature of our computational task. In explicit inference, we derive analytical expressions for $p(d|\theta)$ based on physical models and statistical assumptions. In implicit inference, we face a radically different situation: we have samples $\{(\theta_i, d_i)\}_{i=1}^N$ generated by running our simulator, but \textbf{no functional form} for the underlying distributions.

This creates a profound challenge that traditional statistical methods were not designed to address. Classical density estimation techniques like kernel density estimation or histograms work well in low dimensions but suffer from the \textbf{curse of dimensionality} when applied to the high-dimensional parameter and data spaces typical in cosmology. A weak lensing convergence map might have millions of pixels, while cosmological parameter spaces often exceed 20 dimensions when including nuisance parameters. In such spaces, the number of samples required for traditional methods grows exponentially, quickly exceeding any practical computational budget.

The key insight that makes neural density estimation viable is recognizing that \textbf{cosmological distributions often lie on lower-dimensional manifolds} embedded in high-dimensional spaces. The cosmic web, for instance, exhibits characteristic patterns—filaments, voids, clusters—that represent a tiny fraction of all possible matter distributions. Similarly, the relationship between cosmological parameters and observables is highly structured, constrained by the physics of structure formation. Neural networks excel at learning these implicit lower-dimensional structures from data.

\subsection{Neural Networks as Universal Function Approximators}

The theoretical foundation for using neural networks in density estimation rests on the \textbf{universal approximation theorem}, which guarantees that feedforward neural networks with sufficient width can approximate any continuous function to arbitrary precision. This result, proven independently by Cybenko (1989) and Hornik (1991), provides the mathematical justification for believing that neural networks can learn the complex mappings required for SBI.

A standard feedforward neural network transforms an input $x$ through a sequence of linear transformations and nonlinear activations:
\begin{equation}
    f(x) = W_L \sigma(W_{L-1} \sigma(\cdots \sigma(W_1 x + b_1) \cdots) + b_{L-1}) + b_L
\end{equation}
where $W_\ell$ are weight matrices, $b_\ell$ are bias vectors, and $\sigma(\cdot)$ is a nonlinear activation function (typically ReLU, tanh, or sigmoid). The parameters $\{W_\ell, b_\ell\}$ are learned by minimizing a loss function through gradient-based optimization.

However, the universal approximation theorem alone is insufficient for density estimation. The crucial additional insight is that \textbf{we can use neural networks to parameterize probability distributions} rather than just deterministic functions. Instead of directly outputting density values, neural networks can output the parameters of probability distributions, which can then be trained using maximum likelihood estimation on our simulation samples.

This shift from function approximation to distribution parameterization opens the door to a rich variety of architectures, each designed to capture different aspects of complex probability distributions. The evolution of these methods—from simple mixture models to sophisticated normalizing flows—reflects the field's growing understanding of how to leverage neural network flexibility while maintaining the probabilistic rigor required for Bayesian inference.

\subsection{Mixture Density Networks (MDNs)}

The most natural starting point for neural density estimation builds on the familiar concept of mixture models. Rather than attempting to learn arbitrary probability distributions directly, we begin with the assumption that \textbf{complex distributions can be decomposed into combinations of simple, well-understood components}. This approach, known as Mixture Density Networks (MDNs), represents the earliest attempt to marry the flexibility of neural networks with the mathematical rigor of probability theory.

The core insight behind MDNs is that neural networks excel at learning complex conditional relationships between inputs and outputs. In the SBI context, if we have simulation pairs $\{(\theta_i, d_i)\}_{i=1}^N$, we can train a neural network to predict the parameters of a probability distribution over $d$ given $\theta$. The simplest choice is a mixture of Gaussians:

\begin{equation}
    p(d|\theta) = \sum_{k=1}^K \pi_k(\theta) \mathcal{N}(d | \mu_k(\theta), \Sigma_k(\theta))
\end{equation}

Here, three separate neural networks (or different output heads of a single network) learn to predict the mixture weights $\pi_k(\theta)$, means $\mu_k(\theta)$, and covariance matrices $\Sigma_k(\theta)$ as functions of the cosmological parameters $\theta$. The mixture weights must satisfy $\sum_k \pi_k(\theta) = 1$ and $\pi_k(\theta) \geq 0$, typically enforced through a softmax activation. The covariance matrices must be positive definite, often parameterized through Cholesky decomposition or diagonal approximations.

Training proceeds by maximizing the log-likelihood of the observed simulation data:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \log p(d_i|\theta_i) = \sum_{i=1}^N \log \sum_{k=1}^K \pi_k(\theta_i) \mathcal{N}(d_i | \mu_k(\theta_i), \Sigma_k(\theta_i))
\end{equation}

The power of this approach lies in its \textbf{conceptual simplicity and interpretability}. Each Gaussian component can potentially represent a different "mode" of the cosmic web—one component might capture void-dominated regions, another might represent filamentary structures, and a third could model cluster-rich areas. The mixture weights tell us the relative prevalence of each mode as a function of cosmological parameters.

However, this interpretability comes at a cost. The fundamental limitation of MDNs emerges from the \textbf{finite mixture assumption}: real cosmological distributions often exhibit complexity that cannot be adequately captured by a small number of Gaussian components. Consider the distribution of matter in a cosmic web simulation—the intricate network of filaments, the complex shapes of voids, the hierarchical structure of dark matter halos. These features require either an impractically large number of mixture components or acceptance of significant approximation errors.

Moreover, the choice of $K$ (the number of mixture components) introduces a \textbf{model selection problem} that lacks a principled solution. Too few components lead to underfitting, while too many can cause overfitting and computational instability. This tension between model complexity and tractability motivated the development of more flexible approaches that could adapt their complexity to the data rather than requiring it to be specified a priori.

\subsection{Normalizing Flows}

The limitations of MDNs—their reliance on fixed architectural choices and finite mixture assumptions—motivated a more fundamental approach to neural density estimation. Rather than constraining ourselves to specific parametric families, what if we could \textbf{learn arbitrary transformations} that map complex distributions to simple ones? This insight gave birth to normalizing flows, a revolutionary approach that transforms the density estimation problem into learning invertible mappings.

The conceptual breakthrough underlying normalizing flows rests on a simple mathematical observation: if we can construct an invertible transformation $f$ that maps samples from a complex distribution $p(x)$ to samples from a simple base distribution $p_0(z)$ (typically standard Gaussian), then we can compute exact densities using the change of variables formula from probability theory. This transforms the intractable problem of learning arbitrary densities into the more manageable problem of learning invertible functions.

Consider a sequence of invertible transformations $f = f_L \circ f_{L-1} \circ \cdots \circ f_1$ that maps samples $z \sim p_0(z)$ from a simple base distribution to samples $x = f(z)$ from our target distribution. The probability density of $x$ follows directly from the change of variables theorem:

\begin{equation}
    p(x) = p_0(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right|
\end{equation}

This elegant mathematical relationship provides the foundation for training normalizing flows. Given simulation data $\{x_i\}_{i=1}^N$, we can train the parameters of the transformations $\{f_\ell\}$ by maximizing the log-likelihood:

\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \log p(x_i) = \sum_{i=1}^N \left[ \log p_0(f^{-1}(x_i)) + \log \left| \det \frac{\partial f^{-1}}{\partial x} \right|_{x=x_i} \right]
\end{equation}

The power of this approach lies in its theoretical generality: \textbf{any invertible transformation can, in principle, be learned}. This means that normalizing flows can capture arbitrarily complex distributions, including those with intricate correlation structures, multiple modes, heavy tails, and sharp boundaries—all features commonly found in cosmological simulations.

However, the requirement for invertibility with tractable Jacobian determinants imposes significant architectural constraints. The transformation must be designed such that: (1) the inverse $f^{-1}$ can be computed efficiently, (2) the Jacobian determinant can be evaluated without excessive computational cost, and (3) the function has sufficient expressiveness to capture complex distributions.

These constraints led to several ingenious architectural solutions. \textbf{Coupling flows} address the invertibility requirement by splitting the input dimensions and using one subset to transform the other through additive or affine transformations. \textbf{Autoregressive flows} ensure invertibility by making each output dimension depend only on previous dimensions, allowing sequential computation of both forward and inverse transformations. \textbf{Continuous normalizing flows} use neural ordinary differential equations (ODEs) to define infinitely deep transformations with tractable Jacobians.

The success of normalizing flows in cosmological applications stems from their ability to capture the \textbf{non-linear correlations and complex geometries} inherent in cosmic structure formation. For instance, the relationship between cosmological parameters and weak lensing maps involves intricate dependencies mediated by the non-linear growth of structure, baryonic feedback, and projection effects. Traditional methods like MDNs struggle with such complexity, while normalizing flows can learn these relationships directly from simulation data.

Yet this flexibility comes with computational costs. The requirement to evaluate Jacobian determinants—especially for high-dimensional problems—can make normalizing flows significantly more expensive than simpler alternatives. Moreover, ensuring numerical stability during training requires careful architectural design and regularization techniques. These considerations led researchers to explore even more flexible approaches that could capture distributional complexity without the rigid constraints imposed by invertibility requirements.


\section{From Density Estimation to Bayesian Inference}

Neural density estimation provides the foundation, but \textbf{how do we transform learned distributions into practical Bayesian inference?} The choice of which distribution to learn—posterior $p(\theta|d)$, likelihood $p(d|\theta)$, or likelihood ratios—fundamentally shapes both computational efficiency and scientific workflow. This section examines three principal approaches: Sequential Neural Posterior Estimation (SNPE), Sequential Neural Likelihood Estimation (SNLE), and Neural Ratio Estimation (NRE).

\textbf{Sequential refinement} underlies all three methods: rather than drawing simulations randomly from the prior, we iteratively focus computational resources on parameter regions most relevant for inference, dramatically improving sample efficiency when posteriors occupy small fractions of the prior volume.

\subsection{Sequential Neural Posterior Estimation (SNPE)}

SNPE learns the posterior distribution $p(\theta|d)$ directly by \textbf{reversing the simulation direction}: instead of learning the forward mapping $p(d|\theta)$ defined by our simulator, we learn the inverse $p(\theta|d)$ required for inference.

Given simulation pairs $\{(\theta_i, d_i)\}_{i=1}^N$, we parameterize the posterior using a neural density estimator $q_\phi(\theta|d)$ and optimize:
\begin{equation}
    \phi^* = \arg\max_\phi \sum_{i=1}^N \log q_\phi(\theta_i | d_i)
\end{equation}

For sequential rounds, simulations are drawn from previous posterior approximations rather than the prior, requiring importance weighting:
\begin{equation}
    \mathcal{L}_r = \sum_{i=1}^{N_r} \frac{p(\theta_i)}{q_{r-1}(\theta_i|d_{\text{obs}})} \log q_\phi(\theta_i | d_i)
\end{equation}

Once trained, obtaining posterior samples requires only network forward passes, making SNPE ideal for \textbf{survey-scale applications} like analyzing thousands of galaxy cluster observations or spatial tiles in weak lensing surveys. However, learning complex posterior distributions in high-dimensional parameter spaces requires exponentially growing training data, and the network cannot leverage structural decompositions that might enable more efficient learning across different observations.

\subsection{Sequential Neural Likelihood Estimation (SNLE)}

SNLE preserves the \textbf{modularity of Bayes' theorem} by learning the likelihood $p(d|\theta)$ in the natural simulator direction, then using MCMC for posterior sampling. This separation enables reusing learned likelihoods with different priors and scientific questions.

Training follows the forward simulation direction:
\begin{equation}
    \phi^* = \arg\max_\phi \sum_{i=1}^N \log q_\phi(d_i | \theta_i)
\end{equation}

Inference combines the learned likelihood with arbitrary priors via MCMC:
\begin{equation}
    p(\theta|d_{\text{obs}}) \propto q_\phi(d_{\text{obs}}|\theta) p(\theta)
\end{equation}

A single learned likelihood can be combined with Planck CMB priors, Big Bang nucleosynthesis constraints, or different dark energy models without retraining. The approach supports \textbf{hierarchical decomposition} where separate likelihoods are learned for galaxy shapes, photometric redshifts, and shear correlations. However, MCMC cost per inference eliminates SNPE's amortization advantage, making it less suitable for rapid analysis of many datasets. The method requires high likelihood accuracy since MCMC acceptance ratios are sensitive to approximation errors.

\subsection{Neural Ratio Estimation (NRE)}

NRE recognizes that \textbf{MCMC algorithms fundamentally require likelihood ratios, not absolute densities}. By learning ratios $r(d,\theta) = p(d|\theta)/p(d|\theta_0)$ directly, NRE sidesteps normalization challenges while providing exactly the quantities needed for sampling.

The approach exploits a powerful connection to \textbf{binary classification}: estimating $r(d,\theta)$ is equivalent to training a classifier to distinguish samples $(d,\theta)$ from the target distribution versus samples $(d,\theta_0)$ from a reference distribution. The optimal classifier satisfies:
\begin{equation}
    p(\text{class}=1|d,\theta) = \frac{r(d,\theta)}{r(d,\theta) + 1}, \quad r(d,\theta) = \frac{p(\text{class}=1|d,\theta)}{1-p(\text{class}=1|d,\theta)}
\end{equation}

MCMC then proceeds using learned ratios:
\begin{equation}
    \alpha(\theta' \rightarrow \theta) = \min\left(1, \frac{r(d_{\text{obs}},\theta)}{r(d_{\text{obs}},\theta')}\right)
\end{equation}

Ratio estimation typically requires \textbf{less training data} than full density modeling since classifiers only learn decision boundaries. The approach provides natural uncertainty quantification through classifier confidence and enables direct Bayes factor estimation for model comparison. However, the choice of reference parameter $\theta_0$ critically affects training efficiency and ratio accuracy. Validation becomes complex since traditional density-based diagnostics cannot be applied directly.

\bigskip

\textbf{Method selection guidelines}: SNPE excels for rapid, repeated inference but struggles with high-dimensional parameter spaces. SNLE provides maximum scientific flexibility but requires MCMC per analysis. NRE offers computational efficiency for high-dimensional problems but introduces reference selection complexity. Hybrid approaches increasingly combine complementary strengths of different paradigms.

\section{Dimensionality Reduction for SBI}

Neural density estimators scale poorly with input dimension, while cosmological surveys produce datasets with millions to billions of measurements. This \textbf{curse of dimensionality} necessitates compression that preserves parameter-constraining information while achieving computational tractability.

Traditional analyses compress data into two-point correlation functions by necessity—these were the only statistics with tractable likelihoods. However, this discards the non-Gaussian information that dominates constraining power in nonlinear regimes. The challenge is designing \textbf{learned compression} that preserves precisely the features relevant for parameter inference.


Optimal compression maximizes mutual information $I(f(d), \theta)$ between compressed summaries $f(d)$ and parameters $\theta$, rather than minimizing reconstruction error. Since direct optimization is intractable, \textbf{variational mutual information maximization} (VMIM) optimizes the lower bound:
\begin{equation}
    I(f(d), \theta) \geq \mathbb{E}_{p(d,\theta)}[\log q_\phi(\theta|f(d))] + H(\theta)
\end{equation}
where $q_\phi$ approximates the true posterior.

This approach learns compressions that preserve posterior structure. The data-processing inequality guarantees $I(f(d), \theta) \leq I(d, \theta)$, with equality if and only if $f(d)$ is a \textbf{sufficient statistic}—the theoretical ideal that VMIM approximates.


\section{Robust SBI in Practice}

The transition from theoretical SBI to reliable scientific inference presents a fundamental challenge: \textbf{neural networks are powerful function approximators, but they are not inherently trustworthy}. Unlike explicit inference where errors arise from well-understood approximations, SBI introduces approximation errors through neural network learning that can be difficult to detect and quantify. A neural density estimator might appear to perform well during training yet produce severely biased posteriors when applied to real data.

This challenge becomes acute in cosmology, where inference stakes are high—our understanding of dark matter, dark energy, and fundamental physics depends on accurate parameter constraints from expensive surveys. Poorly validated SBI methods could systematically bias cosmological conclusions across multiple analyses, potentially misleading the field for years.

The core insight driving robust SBI practices is that \textbf{validation must be elevated from an afterthought to a central design principle}. Traditional machine learning validation focuses on predictive accuracy, but scientific inference requires stronger guarantees: confidence that learned distributions accurately capture posterior structure, including tail behavior, correlations, and uncertainty quantification.

\subsection{The Validation Imperative}

\textbf{Why standard machine learning validation fails for scientific inference}: Typical machine learning cares primarily about predictive performance on test data. A neural network achieving 95\% accuracy is considered successful. However, scientific inference requires understanding the full distributional structure of uncertainty—not just point predictions.

Consider an SBI method that systematically underestimates parameter uncertainties by 20\% while achieving excellent test likelihoods. The posterior modes are correct, but this bias would lead cosmologists to claim false precision, potentially causing false detections of new physics or incorrect model rejection. Traditional validation metrics miss this failure mode entirely.

The fundamental issue is that \textbf{point-wise likelihood maximization does not guarantee distributional fidelity}. Networks can achieve high likelihood values while producing posteriors with incorrect variances, missing correlations, or truncated tails. These distributional errors have profound scientific consequences requiring specialized diagnostic tools.

\subsubsection{Simulation-Based Calibration}

The gold standard for SBI validation is \textbf{simulation-based calibration} (SBC), which tests whether learned posterior distributions are statistically consistent with the generative process. The protocol:

\begin{enumerate}
    \item \textbf{Generate calibration dataset}: Sample parameters $\tilde{\theta} \sim p(\theta)$ and observations $\tilde{d} = \mathcal{S}(\tilde{\theta})$
    \item \textbf{Compute posterior samples}: For each $(\tilde{\theta}, \tilde{d})$ pair, obtain samples $\{\theta^{(j)}\}_{j=1}^M$ from $q(\theta|\tilde{d})$
    \item \textbf{Calculate ranks}: For dimension $k$, compute rank $r_k = \sum_{j=1}^M \mathbb{I}[\theta^{(j)}_k < \tilde{\theta}_k]$
    \item \textbf{Test uniformity}: Well-calibrated posteriors produce $r_k \sim \text{Uniform}(0, M)$
\end{enumerate}

Deviations from uniformity reveal specific failures: systematic shifts indicate bias, overdispersion suggests overconfidence, underdispersion reveals underconfidence. \textbf{SBC's power lies in its model-agnostic nature}—testing the fundamental requirement that posteriors correctly represent parameter uncertainty, regardless of neural architecture.

However, SBC requires significant computation. Each test demands hundreds to thousands of posterior evaluations, repeated whenever architecture, training, or simulation models change. This cost often leads practitioners to skip calibration, creating dangerous gaps between development and deployment.

\subsubsection{Coverage Probability Assessment}

While SBC provides a comprehensive distributional test, \textbf{coverage probability assessment} offers a complementary diagnostic that directly tests the reliability of uncertainty quantification. The fundamental principle is intuitive: if our posterior correctly captures parameter uncertainty, then credible intervals should contain the true parameter values at the stated confidence level.

For any confidence level $\alpha \in (0,1)$, a well-calibrated posterior should produce credible intervals that contain the true parameter value exactly $\alpha$ fraction of the time. This seemingly simple requirement has profound implications for scientific inference, as miscalibrated intervals directly translate to incorrect uncertainty estimates in published results.

The diagnostic protocol proceeds systematically. First, we compute $\alpha$-credible intervals $C_\alpha(\tilde{d}) = \{\theta : q(\theta|\tilde{d}) \geq c_\alpha\}$ for each test observation, where the threshold $c_\alpha$ is chosen such that the credible region contains probability mass $\alpha$. Next, we calculate the empirical coverage fraction $\hat{C}_\alpha = \frac{1}{N} \sum_{i=1}^N \mathbb{I}[\tilde{\theta}_i \in C_\alpha(\tilde{d}_i)]$ across all test cases. Finally, we test whether the observed coverage $\hat{C}_\alpha$ statistically agrees with the nominal level $\alpha$.

Coverage tests prove particularly sensitive to \textbf{posterior tail behavior}, which often drives the most scientifically important conclusions. Consider claims of tension between different cosmological datasets or evidence for physics beyond the standard model—these frequently rely on whether parameter estimates fall within or outside credible regions. Underconfident posteriors (coverage $> \alpha$) lead to false precision claims that may result in premature announcements of new physics, while overconfident posteriors (coverage $< \alpha$) cause researchers to dismiss genuine anomalies as statistical fluctuations.

\subsection{Identifying and Mitigating Common Failure Modes}

Experience across multiple SBI applications has revealed characteristic failure patterns that practitioners must guard against. Understanding these failure modes and their mitigation strategies is essential for robust implementation.

\subsubsection{Distribution Shift and Domain Adaptation}

\textbf{SBI's fundamental assumption is that training simulations faithfully represent the data generation process for real observations}. When this breaks down—due to unmodeled systematics, incomplete physics, or computational approximations—distribution shift severely biases inference.

Consider weak lensing analysis where training uses idealized galaxy shapes, but real observations include complex morphologies from blending, detection thresholds, and photometric uncertainties. The learned posterior may be precisely calibrated for simulated data but systematically biased for real observations.

Several diagnostic approaches can identify these troublesome simulation-reality mismatches before they compromise scientific conclusions. \textbf{Two-sample tests} provide the most direct approach by comparing distributions of summary statistics between simulations and observations. When these tests reveal significant differences, they suggest that simulations fail to capture key aspects of the data generation process. However, this approach faces practical challenges: choosing appropriate test statistics requires deep domain expertise, and high-dimensional comparisons suffer from multiple testing corrections that can obscure subtle but important differences.

\textbf{Adversarial validation} offers a more sophisticated diagnostic by training binary classifiers to distinguish between simulated and real data. High classification accuracy indicates that the datasets contain systematically different features, revealing detectable distribution shift. Conversely, random classification performance suggests distributional consistency. This approach can identify subtle shifts that escape traditional statistical tests, though careful interpretation is required to understand which specific features drive the classification decisions.

Perhaps most importantly, \textbf{posterior predictive checks} provide a direct test of the entire inference pipeline's self-consistency. These checks generate synthetic observations from the learned posterior and compare them systematically to real data. Persistent discrepancies reveal either poor posterior calibration or fundamental inadequacies in the simulation models. This approach has the advantage of testing the end-to-end inference process rather than individual components.

When distribution shift is detected, several mitigation strategies can restore robustness. \textbf{Domain adaptation} techniques modify the learned mappings to account for systematic differences between simulations and reality, ranging from simple reweighting schemes that adjust training sample importance to sophisticated adversarial training procedures that learn domain-invariant representations. \textbf{Robust training objectives} modify the standard maximum likelihood loss function to downweight outliers or incorporate distributional constraints, preventing the network from learning spurious correlations specific to the training simulations. Finally, \textbf{transfer learning} can significantly improve performance when computational constraints limit training data quantity by initializing SBI networks using parameters trained on related but higher-fidelity simulations, then fine-tuning on the specific problem of interest.

\subsubsection{Simulator Model Misspecification}

\textbf{Real astrophysical systems inevitably exhibit complexities not captured in computational simulations}, creating a fundamental challenge for simulation-based inference. Dark matter-only simulations miss the crucial effects of baryonic feedback that redistribute matter on galactic scales, finite-resolution codes cannot resolve small-scale physics like star formation and supernovae feedback, and approximate radiative transfer schemes introduce systematic errors in observational predictions. When SBI methods are trained on such misspecified simulators, they learn to infer parameters of an incorrect physical model—potentially leading to biased cosmological conclusions even when the neural network training appears successful.

The challenge of model misspecification extends well beyond simple approximation errors or missing physics. Even different simulation codes implementing ostensibly identical physics can produce subtly different predictions due to variations in numerical schemes, boundary conditions, algorithmic choices, and discretization approaches. An SBI method trained exclusively on one simulation suite may systematically bias results when applied to observational data that more closely resembles the predictions from alternative simulation codes. This "simulation dependence" represents a form of systematic uncertainty that is difficult to quantify without access to multiple independent simulation frameworks.

\textbf{Hierarchical uncertainty modeling} acknowledges simulator limitations by introducing uncertainty terms for unmodeled physics: $d = \mathcal{S}(\theta) + \varepsilon_{\text{model}} + \varepsilon_{\text{obs}}$, where $\varepsilon_{\text{model}}$ represents model uncertainty. The challenge is characterizing $\varepsilon_{\text{model}}$ without access to the "true" process.

\textbf{Ensemble approaches} train multiple networks on different simulation suites or approximations, combining predictions to estimate model uncertainty. Variance across ensemble members provides systematic uncertainty estimates, while ensemble means often prove more robust than individual components.

\subsubsection{Sample Efficiency and Active Learning}

\textbf{Neural density estimators require substantial training data to achieve acceptable accuracy}, yet high-fidelity cosmological simulations represent some of the most computationally expensive scientific calculations ever undertaken. A single state-of-the-art hydrodynamical simulation incorporating realistic galaxy formation physics might require weeks of runtime on modern supercomputers, effectively limiting the total number of training samples to hundreds or thousands—orders of magnitude below the regime where neural networks typically excel. This fundamental tension between data requirements and computational resources creates a critical bottleneck for SBI deployment.

The \textbf{curse of dimensionality} dramatically exacerbates this sample efficiency challenge. Traditional wisdom suggests that the number of samples required for adequate coverage grows exponentially with the dimensionality of parameter space. A realistic 20-dimensional cosmological parameter space—including dark matter properties, baryonic feedback parameters, and observational systematics—would nominally require orders of magnitude more simulations than any conceivable computational budget could provide. This scaling problem threatens to make SBI impractical for high-dimensional inference problems without methodological innovations.

Fortunately, \textbf{sequential design strategies} offer a principled solution by adaptively focusing computational resources on parameter regions most relevant for inference. Rather than drawing training samples uniformly from broad prior distributions, sequential approaches iteratively refine the sampling distribution based on preliminary results from previous rounds. The procedure begins by training an initial SBI network using samples drawn from the prior, then applies this network to observational data to obtain an approximate posterior. New training samples are then drawn from neighborhoods of this approximate posterior, the network is retrained including the new samples with appropriate importance weighting to account for the non-uniform sampling, and the process iterates until convergence criteria are satisfied.

These adaptive approaches can achieve order-of-magnitude improvements in sample efficiency by concentrating expensive simulations precisely where they provide maximum information for parameter inference. However, sequential design requires extremely careful implementation to avoid introducing bias through the adaptive sampling procedure—a failure mode that can be difficult to detect but leads to systematically overconfident posterior estimates. Modern \textbf{active learning criteria} provide principled methods for selecting new simulation points that maximally improve posterior estimates, using either information-theoretic approaches that target points maximizing expected information gain or uncertainty-based strategies that focus on regions where current posterior estimates remain most uncertain.

\subsection{Computational Considerations and Scalability}

Implementing SBI for realistic cosmological applications pushes the boundaries of computational feasibility, requiring careful engineering solutions to bridge the gap between theoretical promise and practical deployment. Modern cosmological surveys generate data at unprecedented scales—the Euclid space telescope will observe billions of galaxies, while the Vera Rubin Observatory will produce petabytes of imaging data. Simultaneously, the high-dimensional nature of both parameter spaces and data products challenges standard neural network training procedures that were designed for more modest problem scales.

\subsubsection{Memory Management and Batch Processing}

The scale of cosmological datasets creates immediate memory bottlenecks that standard machine learning frameworks struggle to handle. Consider weak lensing convergence maps from a typical cosmological simulation: each map might contain millions of pixels representing the projected matter distribution, while a training dataset could include thousands of such maps along with their corresponding high-dimensional parameter vectors. Attempting to load such datasets entirely into memory—as standard neural network training procedures assume—quickly becomes infeasible even on high-memory computing systems.

Several engineering strategies have proven essential for large-scale SBI deployment. \textbf{Streaming data loaders} circumvent memory limitations by reading simulation outputs directly from storage during training, maintaining only small batches in memory at any given time. While this approach imposes additional I/O overhead that can slow training, careful implementation of buffering and prefetching strategies can minimize performance impacts. \textbf{Hierarchical compression} schemes store simulation data in progressively compressed formats, loading full-resolution data only when specific samples are required for training batches. This trades computational overhead for memory efficiency, often proving worthwhile for storage-limited applications. Finally, \textbf{lazy evaluation frameworks} defer expensive data transformations—such as applying observational noise or survey selection functions—until specific samples are actually required, enabling memory-efficient processing of arbitrarily large simulation suites.

\subsubsection{Numerical Stability in High Dimensions}

Beyond memory constraints, neural density estimation in the high-dimensional spaces typical of cosmological applications faces fundamental numerical challenges that can cause training instability, gradient explosion, or convergence to poor local minima. These issues prove particularly acute for normalizing flows, where computing log-likelihood gradients requires stable evaluation of high-dimensional Jacobian determinants—operations that are notoriously sensitive to numerical precision and can easily overflow or underflow in standard floating-point arithmetic.

Modern deep learning practice has developed several \textbf{stabilization techniques} that prove essential for reliable SBI training. \textit{Spectral normalization} constrains the Lipschitz constants of neural network layers, preventing sudden changes in network behavior that can destabilize the training process and lead to gradient explosion. \textit{Early stopping criteria} monitor validation metrics to detect overfitting before it compromises generalization performance, automatically terminating training when continued optimization would hurt test performance. \textit{Learning rate scheduling} adapts optimization hyperparameters based on training progress, carefully balancing convergence speed with numerical stability to ensure robust optimization trajectories.

\subsection{Uncertainty Quantification and Ensemble Methods}

A fundamental challenge in deploying SBI for scientific inference lies in recognizing that \textbf{individual neural networks provide only point estimates of posterior distributions, while rigorous scientific conclusions require understanding the uncertainty in these estimates themselves}. Even when a single network appears well-calibrated during validation testing, it may systematically bias results in subtle ways that are difficult to detect without extensive testing across diverse scenarios. This "uncertainty about uncertainty" represents a crucial gap between machine learning performance metrics and scientific reliability standards.

The core issue stems from the inherent limitations of point estimation in high-stakes scientific applications. Traditional neural network training produces a single set of learned parameters that represents one possible solution to the density estimation problem. However, given finite training data and model approximation errors, many different parameter configurations could achieve similar training performance while producing systematically different posterior estimates. Understanding this model uncertainty—often termed epistemic uncertainty—becomes essential for honest scientific inference that properly acknowledges the limitations of our computational methods.

Ensemble methods provide the most practical approach to epistemic uncertainty quantification by training multiple networks with different random initializations, architectural choices, or training data subsets. The key insight is that the variance across ensemble member predictions provides a natural estimate of our uncertainty about the posterior distribution due to limited training data and model approximation errors. When ensemble members agree closely, we can have higher confidence in the posterior estimate; when they disagree substantially, this signals regions where our conclusions should be interpreted with greater caution.

\textbf{Deep ensembles} implement this principle by training multiple networks independently and combining their predictions through averaging or more sophisticated aggregation schemes. Extensive empirical evaluation has demonstrated that this approach consistently improves calibration compared to single networks while providing uncertainty estimates that correlate well with prediction accuracy. The ensemble approach proves particularly valuable for identifying problematic regions of parameter or data space where individual networks might confidently provide incorrect estimates.

\textbf{Bayesian neural networks} offer a more theoretically principled alternative by placing probability distributions over network parameters rather than learning deterministic point estimates. This approach provides theoretical guarantees about uncertainty quantification that deterministic methods cannot match, explicitly representing our epistemic uncertainty through the parameter posterior distribution. However, this theoretical elegance comes at substantial computational cost, as inference in Bayesian neural networks requires either expensive sampling procedures or complex variational approximations that can be difficult to implement and validate.

The transition from traditional likelihood-based methods to simulation-based inference represents far more than a technical upgrade—it embodies a \textbf{fundamental paradigm shift toward simulation-driven science} that embraces the full complexity of cosmological physics without forcing it into analytically tractable approximations. This transformation enables cosmology to fully leverage decades of investment in sophisticated simulation codes while maintaining the rigorous statistical framework of Bayesian analysis.

However, this powerful new capability comes with profound new responsibilities. The validation and uncertainty quantification challenges we have discussed require sustained attention from the cosmological community to ensure that the remarkable flexibility of modern machine learning serves the goals of robust scientific inference rather than introducing subtle biases that could mislead our understanding of the universe. The stakes are too high—and the computational investments too substantial—to deploy these methods without the comprehensive validation frameworks that scientific applications demand.

\section{Practical Resources}

\subsection{Software Frameworks}
\begin{itemize}
    \item \texttt{sbi}: \url{https://github.com/mackelab/sbi} - Comprehensive Python library implementing SNPE, SNLE, and NRE with extensive validation tools
    \item \texttt{lampe}: Likelihood-free AMortized Posterior Estimation with PyTorch backend and GPU acceleration
    \item \texttt{pydelfi}: Density Estimation Likelihood-Free Inference with focus on normalizing flows
    \item \texttt{sbibm}: Simulation-Based Inference Benchmark providing standardized problems for method comparison
\end{itemize}

\subsection{Specialized Tools}
\begin{itemize}
    \item \texttt{corner}: Posterior visualization and summary statistics
    \item \texttt{arviz}: Bayesian inference diagnostics and visualization
    \item \texttt{chainconsumer}: MCMC chain analysis and comparison tools
\end{itemize}


\section{Conclusion}

\section*{Acknowledgements}
Acknowledgements should follow immediately after the conclusion.

% TODO: include author contributions
\paragraph{Author contributions}
This is optional. If desired, contributions should be succinctly described in a single short paragraph, using author initials.

% TODO: include funding information
\paragraph{Funding information}
Authors are required to provide funding information, including relevant agencies and grant numbers with linked author's initials. Correctly-provided data will be linked to funders listed in the \href{https://www.crossref.org/services/funder-registry/}{\sf Fundref registry}.


% \begin{appendix}
% \numberwithin{equation}{section}

% \section{First appendix}
% Add material which is better left outside the main text in a series of Appendices labeled by capital letters.


%%%%%%%%% END TODO: CONTENTS


%%%%%%%%%% TODO: BIBLIOGRAPHY
% Provide your bibliography here. You have two options:
\bibliography{sbi.bib}

%%%%%%%%%% END TODO: BIBLIOGRAPHY


\end{document}
