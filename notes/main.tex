\documentclass[11pt,a4paper]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{cleveref}

% Page geometry
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
    headheight=24pt
}

% Colors
\definecolor{leshouches}{RGB}{25,55,95}
\definecolor{lightgray}{RGB}{240,240,240}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{leshouches}{\small\textsc{Les Houches School of Physics}}}
\fancyhead[R]{\textcolor{leshouches}{\small Lecture \thelecturenumber}}
\fancyfoot[C]{\textcolor{leshouches}{\thepage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{leshouches}\leaders\hrule height \headrulewidth\hfill}}

% Section formatting
\titleformat{\section}
{\Large\bfseries\color{leshouches}}
{\thesection}{1em}{}

\titleformat{\subsection}
{\large\bfseries\color{leshouches}}
{\thesubsection}{1em}{}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{remark}{Remark}

% Custom commands
\makeatletter
\newcommand{\lecturetitle}[1]{\gdef\@lecturetitle{#1}}
\newcommand{\lecturer}[1]{\gdef\@lecturer{#1}}
\newcommand{\lecturedate}[1]{\gdef\@lecturedate{#1}}
\newcommand{\lecturenumber}[1]{\gdef\@lecturenumber{#1}\gdef\thelecturenumber{#1}}

% Default values
\gdef\@lecturetitle{Lecture Title}
\gdef\@lecturer{Lecturer Name}
\gdef\@lecturedate{\today}
\gdef\@lecturenumber{1}

% Title formatting
\renewcommand{\maketitle}{
    \begin{center}
        {\Large\textcolor{leshouches}{\textbf{Les Houches School of Physics}}}\\[0.5em]
        {\large Summer School on Dark Universe, 2025}\\[1.5em]
        {\huge\textbf{\@lecturetitle}}\\[0.5em]
        {\large Lecture \@lecturenumber}\\[1em]
        {\large\textit{\@lecturer}}\\[0.5em]
        {\@lecturedate}
    \end{center}
    \vspace{1em}
    \hrule
    \vspace{1.5em}
}
\makeatother

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=leshouches,
    citecolor=leshouches,
    urlcolor=leshouches,
    pdftitle={Simulation-Based Inference for Cosmology - Les Houches 2025},
    pdfauthor={François Lanusse, CNRS/CEA Paris-Saclay}
}

% Document begins here
\begin{document}

% Set lecture information
\lecturetitle{Simulation-Based Inference for Cosmology}
\lecturer{François Lanusse, CNRS/CEA Paris-Saclay}
\lecturedate{Summer 2025}
\lecturenumber{1}

\maketitle

\section*{Introduction}

Simulation-Based Inference (SBI) addresses a fundamental challenge in modern cosmological data analysis: performing Bayesian inference when the likelihood function $p(d|\theta)$ is intractable or computationally prohibitive to evaluate. Traditional approaches rely on analytical likelihood expressions or approximations that become inadequate for complex, high-dimensional data such as cosmic web fields, galaxy clustering, or gravitational lensing maps.

The key insight of SBI is to \textbf{replace explicit likelihood} evaluations required by traditional MCMC, \textbf{with an implicit likelihood learned from simulations}. In other words, rather than deriving the likelihood of the data $p(d|\theta)$ analytically, simulations can be used to generate synthetic training data that captures the relationship between parameters $\theta$ and observations $d$. Neural networks can then be used to approximate the relevant distributions (whether the posterior $p(\theta|d)$, likelihood $p(d|\theta)$, or likelihood ratios) directly from this simulation data.

This approach has proven particularly powerful in cosmology, where sophisticated N-body simulations and hydrodynamical codes can generate realistic synthetic observations, but the underlying physics is too complex for tractable likelihood expressions. SBI enables inference over high-dimensional parameter spaces using the full information content of complex datasets, moving beyond traditional summary statistics.

In these lecture notes, we will cover the connection between SBI and other forms of Bayesian inference, the theoretical foundations of neural density estimation powering SBI, and its practical applications in cosmological inference problems. We will also discuss the challenges and best practices for implementing SBI in real-world scenarios.


\section{MCMC, SBI, Full Field Inference... What is the difference?}

\subsection{Fundamental notations and definitions for Bayesian Inference}

Let us start by establishing the standard framework for Bayesian inference. Let $\theta \in \Theta \subseteq \mathbb{R}^p$ denote a vector of parameters of interest, and $d \in \mathcal{D}$ represent observed data. The fundamental quantities in Bayesian inference are:

\begin{itemize}
    \item \textbf{Prior distribution}: $p(\theta)$ encodes our beliefs about parameters before observing data
    \item \textbf{Likelihood function}: $p(d|\theta)$ specifies the probability of observing data $d$ given parameters $\theta$
    \item \textbf{Posterior distribution}: $p(\theta|d)$ represents our updated beliefs about parameters after observing data
    \item \textbf{Evidence (marginal likelihood)}: $p(d) = \int p(d|\theta)p(\theta) \, d\theta$ normalizes the posterior
\end{itemize}

Given these definitions, the famous Bayes' theorem connects these quantities according to:
\begin{equation}
    \boxed{p(\theta|d) = \frac{p(d|\theta)p(\theta)}{p(d)}}
\end{equation}

No matter the inference strategy, these definitions remain the same. SBI will differ from other Bayesian inference techniques only in the strategy used to estimate these distributions. 

\subsection{Explicit Inference}

\textbf{Explicit inference} forms the foundation of classical Bayesian computation and encompasses all approaches where the \textbf{likelihood function $p(d|\theta)$ can be evaluated point-wise} for any given combination of data $d$ and parameters $\theta$. This seemingly simple requirement enables the entire machinery of Markov Chain Monte Carlo (MCMC) algorithms that have dominated Bayesian inference for decades.

To understand why likelihood evaluation is crucial, consider how MCMC algorithms explore the posterior distribution. Whether using the Metropolis-Hastings algorithm, Gibbs sampling, or more sophisticated variants, these methods fundamentally rely on computing \textbf{likelihood ratios} to make accept-reject decisions. At each step, a proposal $\theta'$ is compared to the current state $\theta$ through the ratio:
\begin{equation}
    r = \frac{p(\theta'|d)}{p(\theta|d)} = \frac{p(d|\theta')p(\theta')}{p(d|\theta)p(\theta)}
\end{equation}
Without the ability to evaluate the likelihoods $p(d|\theta)$ and $p(d|\theta')$ at two different points in parameter space, this ratio cannot be computed, and the MCMC chain cannot advance. This fundamental dependency explains why explicit inference has been the dominant paradigm in computational statistics.

In practice, evaluating likelihoods for raw cosmological data presents a fundamental challenge. Consider a weak lensing survey observing shear measurements $\gamma_{i}$ for millions of galaxies across the sky. The likelihood $p(\{\gamma_i\}|\theta)$ for these raw measurements involves complex correlations due to cosmic structure, survey geometry, and measurement uncertainties—making analytical evaluation intractable. The same problem arises for galaxy positions, CMB temperature maps, or any high-dimensional cosmological dataset.

This tractability issue drove cosmologists to adopt \textbf{summary statistics} as an ingenious workaround. Instead of working with millions of individual measurements, we compress the data into a manageable set of statistics for which analytical likelihoods can be derived. The \textbf{two-point correlation function} became the workhorse of cosmological analysis precisely because its likelihood could be written down. For weak lensing, shear measurements from millions of galaxies are compressed into correlation functions $\xi_+(\theta)$ and $\xi_-(\theta)$, whose likelihood can be modeled as multivariate Gaussian:
\begin{equation}
    p(\xi_{\text{obs}}|\theta) = \mathcal{N}(\xi_{\text{obs}}; \xi_{\text{theory}}(\theta), C)
\end{equation}
where $\xi_{\text{theory}}(\theta)$ comes from analytical models (linear perturbation theory, halo models) and $C$ represents the covariance matrix. This Gaussian assumption is justified by the Central Limit Theorem when averaging over many independent modes, making the likelihood both tractable and well-understood.

This framework has enabled precise cosmological parameter constraints from Large Scale Structure surveys like DES, KiDS, and HSC. However, it comes at the cost a fundamental limitation: potential \textbf{information loss}. By compressing millions of measurements into a handful of correlation function bins, we may discard a significant amount of information contained in higher-order statistics, non-Gaussian features, and spatial correlations that cannot be captured by two-point functions, which are only theoretically optimal for the analysis of Gaussian fields.

To access the \textit{full information content} of a given survey while maintaining explicit inference, we must abandon summary statistics and instead model the measured fields in high dimension and in all their complexity, which is usualy referred to as \textbf{full-field inference}. This will require constructing complex models that represent the \textbf{complete data generation process} behind the observations, in other words, a forward simulation of the measured high-dimensional data.

Performing explicit inference over such models is possible but requires the introduction of a set of \textbf{latent variables} $z$ representing unobserved quantities like the true underlying cosmic fields. \textbf{The generative model becomes hierarchical} and can be expressed as:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    z|\theta &\sim p(z|\theta) \quad \text{(latent cosmic field)} \\
    d|z &\sim p(d|z) \quad \text{(observation process)}
\end{align}

The joint posterior over both parameters and latent variables is:
\begin{equation}
    p(\theta, z|d) \propto p(d|z)p(z|\theta)p(\theta)
\end{equation}

This framework remains \textbf{explicitly tractable} as long as each component can be evaluated: $p(d|z)$ models the observation process (e.g., Poisson noise, instrumental systematics), $p(z|\theta)$ represents the physical field generation process, and $p(\theta)$ encodes prior knowledge. Compared to summary-statistics-based explicit inference, we now need to infer the set of latent variable $z$ in addition to the comological parameters $\theta$ which can become very challening whe $z$ is high-dimensional. 

A paradigmatic example is the \textbf{BORG algorithm} (Bayesian Origin Reconstruction from Galaxies, see Florent's lecture in this school). BORG performs inference on the initial density field of the universe given observed galaxy positions. The hierarchical model structure is:
\begin{align}
    \theta &\sim p(\theta) \quad \text{(cosmological parameters)} \\
    \delta_0 &\sim \mathcal{N}(0, P(\theta)) \quad \text{(initial density field)} \\
    \delta(z) &= \mathcal{F}[\delta_0, \theta] \quad \text{(gravitational evolution)} \\
    n_g &= \text{Poisson}(\rho_g[\delta(z)]) \quad \text{(galaxy counts)}
\end{align}

Here, $\mathcal{F}$ represents forward modeling through Lagrangian Perturbation Theory (LPT), and $\rho_g$ is the biased galaxy density field. Crucially, even though $\mathcal{F}$ involves complex physics, the likelihood remains explicit because: (1) LPT provides an analytical (though approximate) mapping from initial to final conditions, (2) all intermediate variables ($\delta_0$, $\delta(z)$) are tracked explicitly, and (3) the observation model $p(n_g|\delta(z))$ is analytically tractable. BORG can simultaneously reconstruct the cosmic web and constrain cosmological parameters while utilizing the full spatial information in galaxy surveys.

From an information-theoretic perspective, explicit hierarchical inference represents the \textbf{theoretically optimal approach} to parameter estimation. By modeling the complete generative process without information-losing compression, these methods can achieve the theoretical minimum possible uncertainties (the Cramér-Rao bound). 

However, this theoretical optimality comes with important practical challenges. 

The first is \textbf{computational scaling}: the dimensionality of latent variable spaces grows rapidly with field resolution. A modest $128^3$ cosmic web simulation involves $\sim 2 \times 10^6$ latent variables, making standard MCMC inefficient. High-dimensional sampling requires sophisticated algorithms like Hamiltonian Monte Carlo (HMC) that can explore the posterior efficiently.

The second challenge involves \textbf{differentiability requirements}: algorithms like HMC require computing gradients of the log-posterior with respect to all variables, including the high-dimensional latent field. This necessitates that the forward model $p(z|\theta)$ be differentiable, a strong constraint that excludes many realistic simulation codes.

The third challenge is \textbf{model fidelity}: explicit models must make approximations to remain tractable. BORG's use of LPT, while differentiable, becomes inaccurate in highly nonlinear regimes. The fundamental tension between model complexity and computational tractability limits the realism achievable in explicit frameworks.

\bigskip

Wouldn't it be nice if we could perform Full-Field inference without running into these difficulties intrinsic to high-dimensional inference?

\subsection{Implicit Inference}

\textbf{Implicit inference} becomes necessary when the forward model cannot be decomposed into tractable probabilistic components, making the likelihood $p(d|\theta)$ effectively unevaluable. This situation arises when simulations involve complex, nonlinear processes that defy analytical description or when computational constraints make likelihood evaluation prohibitively expensive.

\textbf{The Black-Box Simulation Regime}: Consider a complex forward model $\mathcal{S}(\theta, \epsilon)$ where $\epsilon$ represents stochastic elements that are not tracked or controllable (e.g., random number generator seeds, stochastic subgrid physics implementations, Monte Carlo sampling within the simulation). The relationship between parameters $\theta$ and observations $d$ is implicitly defined through:
\begin{equation}
    d = \mathcal{S}(\theta, \epsilon) + \text{noise}
\end{equation}

Examples include:
\begin{itemize}
    \item Hydrodynamical simulations with complex baryonic physics and feedback prescriptions
    \item Galaxy formation models with stochastic star formation and merger histories  
    \item Full radiative transfer codes with Monte Carlo photon transport
    \item End-to-end instrument simulators with realistic systematics
\end{itemize}

In this regime, the likelihood $p(d|\theta)$ cannot be computed directly because:
\begin{enumerate}
    \item The mapping $\mathcal{S}$ is not analytically tractable
    \item The stochastic elements $\epsilon$ are not marginalized over in closed form
    \item Each likelihood evaluation would require many expensive simulations to estimate the integral over $\epsilon$
\end{enumerate}

\textbf{The SBI Solution}: Despite these challenges, we can generate samples from the joint distribution $p(d, \theta)$ by the simple procedure:
\begin{enumerate}
    \item Sample parameters $\theta \sim p(\theta)$ from the prior
    \item Run forward simulation $d \sim p(d|\theta)$ via $\mathcal{S}(\theta, \epsilon)$
\end{enumerate}

\textbf{Simulation-Based Inference} exploits this sampling capability by training neural networks to approximate the distributions needed for Bayesian inference—whether the posterior $p(\theta|d)$, likelihood $p(d|\theta)$, or likelihood ratios—directly from pairs $\{(\theta_i, d_i)\}_{i=1}^N$ generated by this procedure.

\textbf{Key Advantages}: Handles arbitrarily complex simulators, scales to high-dimensional parameter and data spaces, leverages full information content without summary statistics.

\textbf{Key Challenges}: Requires careful neural architecture design, simulation efficiency becomes critical, validation requires specialized techniques since exact answers are unknown.

This \textbf{paradigm shift from explicit evaluation to implicit learning} enables inference in previously intractable scenarios while preserving the principled foundation of Bayesian statistics. The next section explores the neural density estimation techniques that make this approach possible.

\section{Neural Density Estimation}

The core challenge in implicit inference is learning probability distributions from samples rather than analytical expressions. This section introduces the neural network machinery that enables SBI by approximating complex, high-dimensional probability densities from simulation data.

\subsection{The Density Estimation Problem}

In traditional statistics, we often work with known parametric families (e.g., Gaussian, exponential) where the functional form is specified and we only need to estimate parameters. For SBI, we face a fundamentally different challenge: **learning the shape of unknown probability distributions** from samples generated by black-box simulators.

Given samples $\{x_i\}_{i=1}^N$ drawn from an unknown distribution $p(x)$, density estimation seeks to construct an approximation $\hat{p}(x)$ that captures the underlying probability structure. In the SBI context, these samples come from running forward simulations: $x_i = \mathcal{S}(\theta_i, \epsilon_i)$ where $\theta_i$ are sampled parameters and $\epsilon_i$ represent simulation randomness.

\subsection{Neural Networks as Universal Approximators}

Neural networks provide the flexibility needed for this task through their **universal approximation capabilities**. A feedforward neural network with sufficient width can approximate any continuous function to arbitrary precision, making them natural candidates for density estimation.

The basic building block is a multilayer perceptron:
\begin{equation}
    f(x) = W_L \sigma(W_{L-1} \sigma(\cdots \sigma(W_1 x + b_1) \cdots) + b_{L-1}) + b_L
\end{equation}
where $W_\ell$ are weight matrices, $b_\ell$ are bias vectors, and $\sigma(\cdot)$ is a nonlinear activation function. The network learns optimal parameters through gradient-based optimization on training data.

For density estimation, the key insight is to use neural networks to parameterize probability distributions, then train them to maximize the likelihood of observed samples.

\subsection{Mixture Density Networks (MDNs)}

The conceptually simplest approach to neural density estimation uses **mixture models** where neural networks predict the parameters of a mixture of simple distributions. For a mixture of $K$ Gaussians:

\begin{equation}
    p(x|\theta) = \sum_{k=1}^K \pi_k(\theta) \mathcal{N}(x | \mu_k(\theta), \sigma_k(\theta))
\end{equation}

Here, neural networks $\pi_k(\cdot)$, $\mu_k(\cdot)$, and $\sigma_k(\cdot)$ output the mixture weights, means, and standard deviations respectively as functions of conditioning variables $\theta$. The networks are trained by maximizing the log-likelihood:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \log p(x_i|\theta_i)
\end{equation}

**Advantages**: Interpretable components, stable training, works well for unimodal or simple multimodal distributions.

**Limitations**: Limited flexibility for complex distributions, requires manual specification of mixture components $K$.

\subsection{Normalizing Flows}

For greater flexibility, **normalizing flows** learn invertible transformations between a simple base distribution (e.g., standard Gaussian) and the target distribution. The key idea is to apply a sequence of invertible functions $f = f_L \circ \cdots \circ f_1$ that transform samples $z \sim p_0(z)$ to $x = f(z)$.

The probability density follows from the change of variables formula:
\begin{equation}
    p(x) = p_0(z) \left| \det \frac{\partial f^{-1}}{\partial x} \right| = p_0(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right|
\end{equation}

Each transformation $f_i$ must be invertible with tractable Jacobian determinant. Common architectures include:
\begin{itemize}
    \item **Coupling flows**: Split dimensions and use neural networks to transform subsets
    \item **Autoregressive flows**: Transform dimensions sequentially using autoregressive dependencies
    \item **Continuous normalizing flows**: Use neural ODEs for infinite-depth transformations
\end{itemize}

**Advantages**: High expressiveness, exact likelihood computation, efficient sampling via forward pass.

**Limitations**: Architectural constraints for invertibility, computational cost of Jacobian determinants.

\subsection{Advanced Methods}

Recent developments have expanded the toolkit for neural density estimation:

\begin{itemize}
    \item **Score matching**: Instead of learning $p(x)$ directly, learn the score function $s(x) = \nabla_x \log p(x)$. The score characterizes the distribution and can be learned without normalization constants.
    
    \item **Diffusion models**: Model the gradual denoising process from pure noise to data samples. These have shown remarkable success in generative modeling and are increasingly used for density estimation.
    
    \item **Neural spline flows**: Use neural networks to parameterize monotonic spline functions, providing a good balance between flexibility and computational efficiency.
\end{itemize}

The choice of method depends on the specific requirements: MDNs for interpretability, normalizing flows for exact likelihoods, and advanced methods for maximum flexibility. In the next section, we explore how these density estimation techniques enable the three main approaches to SBI.

\section{From Density Estimation to Bayesian Inference}

\subsection{Sequential Neural Posterior Estimation (SNPE)}
Learn the posterior $p(\theta|x)$ directly using neural density estimation.

\subsection{Sequential Neural Likelihood Estimation (SNLE)}
Learn the likelihood $p(x|\theta)$ and use MCMC for posterior sampling.

\subsection{Neural Ratio Estimation (NRE)}
Learn likelihood ratios $r(x,\theta) = p(x|\theta) / p(x|\theta_0)$ for efficient inference.

\begin{remark}
The \texttt{sbi} package provides implementations of all these methods with a unified interface.
\end{remark}

\section{Dimensionality Reduction for SBI}

\subsection{Beyond Two-Point Functions}
Capturing non-Gaussian information in cosmological fields requires going beyond traditional summary statistics.

\subsection{Compression Strategies}
\begin{itemize}
    \item Scattering transforms
    \item Neural compression
    \item Information-maximizing summaries
\end{itemize}

Key principle: Preserve information relevant for inference while reducing dimensionality.

\section{Robust SBI in Practice}

\subsection{Key Considerations}
\begin{enumerate}
    \item \textbf{Frozen summarizers}: Keep feature extraction fixed during inference
    \item \textbf{Sample efficiency}: Active learning and sequential strategies
    \item \textbf{Ensemble methods}: Multiple networks for uncertainty quantification
    \item \textbf{Validation}: Simulation-based calibration and coverage tests
\end{enumerate}

\subsection{Common Pitfalls}
\begin{itemize}
    \item Distribution shift between simulations and real data
    \item Model misspecification
    \item Insufficient simulation fidelity
\end{itemize}

\section{Practical Resources}

\subsection{Software}
\begin{itemize}
    \item \texttt{sbi}: \url{https://github.com/mackelab/sbi}
    \item \texttt{lampe}: Likelihood-free AMortized Posterior Estimation
    \item \texttt{pydelfi}: Density Estimation Likelihood-Free Inference
\end{itemize}

\subsection{Key References}
\begin{itemize}
    \item Cranmer et al. (2020), ``The frontier of simulation-based inference''
    \item Papamakarios et al. (2019), ``Neural Density Estimation: A Review''
    \item Alsing \& Wandelt (2019), ``Generalized massive optimal data compression''
    \item Jeffrey et al. (2020), ``Likelihood-free inference with neural compression''
\end{itemize}

\end{document}